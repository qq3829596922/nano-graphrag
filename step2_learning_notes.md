# GraphRAGå­¦ä¹ ç³»åˆ— - ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬å¤„ç†ç³»ç»Ÿ

## ğŸ“– å­¦ä¹ ç›®æ ‡

è¿™ä¸€æ­¥æˆ‘ä»¬æ·±å…¥ç†è§£GraphRAGä¸­çš„**æ–‡æœ¬åˆ†å—ï¼ˆText Chunkingï¼‰**æœºåˆ¶ï¼Œè¿™æ˜¯æ„å»ºçŸ¥è¯†å›¾è°±çš„å…³é”®ç¬¬äºŒæ­¥ã€‚

### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

1. **ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ**
   - LLMæœ‰ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼ˆå¦‚GPT-4çš„8K-32K tokensï¼‰
   - é•¿æ–‡æ¡£éœ€è¦åˆ‡åˆ†æˆé€‚åˆå¤„ç†çš„å°æ®µè½
   - åˆ†å—è´¨é‡ç›´æ¥å½±å“åç»­å®ä½“æå–å’Œå…³ç³»è¯†åˆ«çš„æ•ˆæœ

2. **åˆ†å—çš„æŒ‘æˆ˜**
   - å¦‚ä½•ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Ÿ
   - å¦‚ä½•é˜²æ­¢å…³é”®ä¿¡æ¯åœ¨è¾¹ç•Œä¸¢å¤±ï¼Ÿ
   - å¦‚ä½•å¹³è¡¡åˆ†å—å¤§å°å’Œå¤„ç†æ•ˆç‡ï¼Ÿ

## ğŸ”§ æŠ€æœ¯å®ç°

### æ ¸å¿ƒç±»ç»“æ„

```
TextProcessor (ä¸»å¤„ç†å™¨)
â”œâ”€â”€ SeparatorSplitter (åˆ†éš”ç¬¦åˆ†å‰²å™¨)
â”œâ”€â”€ ChunkResult (åˆ†å—ç»“æœæ•°æ®ç»“æ„)
â””â”€â”€ ä¸¤ç§åˆ†å—ç­–ç•¥:
    â”œâ”€â”€ chunking_by_token_size() (Tokenåˆ†å—)
    â””â”€â”€ chunking_by_separators() (åˆ†éš”ç¬¦åˆ†å—)
```

### å…³é”®æŠ€æœ¯ç‰¹æ€§

1. **ç²¾ç¡®Tokenè®¡ç®—**
   - ä½¿ç”¨tiktokenåº“ç²¾ç¡®è®¡ç®—tokenæ•°é‡
   - æ”¯æŒå¤šç§LLMæ¨¡å‹çš„ç¼–ç æ ¼å¼
   - æ‰¹é‡ç¼–ç ä¼˜åŒ–æ€§èƒ½

2. **æ™ºèƒ½é‡å æœºåˆ¶**
   - æ»‘åŠ¨çª—å£é¿å…ä¿¡æ¯ä¸¢å¤±
   - å¯é…ç½®é‡å å¤§å°
   - ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§

3. **å¤šå±‚åˆ†éš”ç¬¦ç­–ç•¥**
   ```python
   ä¼˜å…ˆçº§åˆ†éš”ç¬¦:
   "\n\n"    # æ®µè½åˆ†éš”ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
   "\n"      # è¡Œåˆ†éš”
   "ã€‚"/"."  # å¥å·
   "ï¼"/"!"  # æ„Ÿå¹å·
   "ï¼Ÿ"/"?"  # é—®å·
   " "       # ç©ºæ ¼ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
   ```

## ğŸƒ è¿è¡Œæ­¥éª¤

### 1. å®‰è£…ä¾èµ–
```bash
pip install tiktoken
```

### 2. è¿è¡Œæ¼”ç¤º
```bash
python step2_text_processing.py
```

### 3. è§‚å¯Ÿè¾“å‡ºç»“æœ
- Tokenåˆ†å— vs åˆ†éš”ç¬¦åˆ†å—çš„å¯¹æ¯”
- åˆ†å—å¤§å°å’Œé‡å çš„æ•ˆæœ
- ä¸å­˜å‚¨ç³»ç»Ÿçš„é›†æˆ

## ğŸ“Š å­¦ä¹ é‡ç‚¹

### 1. Tokenåˆ†å—ç­–ç•¥
```python
# æ»‘åŠ¨çª—å£æœºåˆ¶
step_size = max_token_size - overlap_token_size
for start in range(0, len(tokens), step_size):
    chunk = tokens[start:start + max_token_size]
```

**ç‰¹ç‚¹ï¼š**
- âœ… ç²¾ç¡®æ§åˆ¶å¤§å°
- âœ… å¤„ç†é€Ÿåº¦å¿«
- âŒ å¯èƒ½æˆªæ–­è¯­ä¹‰

### 2. åˆ†éš”ç¬¦åˆ†å—ç­–ç•¥
```python
# æ™ºèƒ½è¯­ä¹‰è¾¹ç•Œè¯†åˆ«
splitter = SeparatorSplitter(
    separators=separator_tokens,
    chunk_size=max_token_size,
    chunk_overlap=overlap_token_size,
)
```

**ç‰¹ç‚¹ï¼š**
- âœ… ä¿æŒè¯­ä¹‰å®Œæ•´
- âœ… æ™ºèƒ½è¾¹ç•Œå¯¹é½
- âŒ å¤„ç†ç¨æ…¢

### 3. æ•°æ®ç»“æ„è®¾è®¡
```python
@dataclass
class ChunkResult:
    content: str          # åˆ†å—å†…å®¹
    token_count: int      # Tokenæ•°é‡
    chunk_index: int      # åœ¨åŸæ–‡æ¡£ä¸­çš„é¡ºåº
    doc_id: str          # åŸå§‹æ–‡æ¡£ID
    chunk_id: str        # åˆ†å—å”¯ä¸€ID
```

## ğŸ” ä»£ç åˆ†æé‡ç‚¹

### 1. æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
# æ‰¹é‡ç¼–ç æé«˜æ€§èƒ½
tokens_list = self.encoder.encode_batch(contents, num_threads=16)

# æ‰¹é‡è§£ç 
chunk_contents = self.encoder.decode_batch(chunk_tokens_list)
```

### 2. é‡å æœºåˆ¶å®ç°
```python
def _add_overlap(self, chunks):
    result = [chunks[0]]  # ç¬¬ä¸€ä¸ªåˆ†å—ä¸éœ€è¦é‡å 
    for i in range(1, len(chunks)):
        overlap_tokens = chunks[i-1][-self._chunk_overlap:]
        new_chunk = overlap_tokens + chunks[i]
        result.append(new_chunk)
```

### 3. ä¸å­˜å‚¨ç³»ç»Ÿé›†æˆ
```python
async def save_chunks_to_storage(self, chunks, storage_dir):
    from step1_document_processing import JsonKVStorage
    chunk_storage = JsonKVStorage(namespace="text_chunks", ...)
    # ä¿å­˜åˆ°å­˜å‚¨ç³»ç»Ÿ
```

## ğŸ§ª å®éªŒå»ºè®®

### 1. å‚æ•°è°ƒä¼˜å®éªŒ
```python
# å®éªŒä¸åŒçš„åˆ†å—å¤§å°
processor = TextProcessor(
    default_chunk_size=800,   # è¯•è¯• 400, 800, 1200
    default_overlap_size=80   # è¯•è¯• 50, 100, 150
)
```

### 2. åˆ†éš”ç¬¦å®šåˆ¶å®éªŒ
```python
# å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦ä¼˜å…ˆçº§
custom_separators = [
    "\n\n",     # æ®µè½ä¼˜å…ˆ
    "ã€‚",       # ä¸­æ–‡å¥å·
    ".",        # è‹±æ–‡å¥å·
    " ",        # ç©ºæ ¼
]
```

### 3. æ€§èƒ½å¯¹æ¯”å®éªŒ
- æµ‹è¯•ä¸åŒæ–‡æ¡£é•¿åº¦çš„å¤„ç†æ—¶é—´
- æ¯”è¾ƒä¸¤ç§åˆ†å—ç­–ç•¥çš„æ•ˆæœ
- è§‚å¯Ÿé‡å å¤§å°å¯¹ç»“æœè´¨é‡çš„å½±å“

## ğŸ”— ä¸åŸå§‹é¡¹ç›®çš„å¯¹åº”å…³ç³»

| æˆ‘ä»¬çš„å®ç° | åŸå§‹é¡¹ç›®ä½ç½® | åŠŸèƒ½å¯¹åº” |
|-----------|-------------|----------|
| `TextProcessor` | `nano_graphrag/_op.py` | æ–‡æœ¬å¤„ç†æ ¸å¿ƒé€»è¾‘ |
| `chunking_by_token_size()` | `chunking_by_token_size()` | Tokenåˆ†å—æ–¹æ³• |
| `SeparatorSplitter` | `nano_graphrag/_splitter.py` | åˆ†éš”ç¬¦åˆ†å‰²å™¨ |
| `ChunkResult` | åˆ†å—æ•°æ®ç»“æ„ | ç»“æœæ ¼å¼å®šä¹‰ |

## â“ æ€è€ƒé¢˜

1. **ä¸ºä»€ä¹ˆéœ€è¦é‡å æœºåˆ¶ï¼Ÿ**
   - é˜²æ­¢é‡è¦ä¿¡æ¯åœ¨åˆ†å—è¾¹ç•Œä¸¢å¤±
   - ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
   - æé«˜åç»­å¤„ç†çš„å‡†ç¡®æ€§

2. **å¦‚ä½•é€‰æ‹©åˆé€‚çš„åˆ†å—å¤§å°ï¼Ÿ**
   - è€ƒè™‘LLMçš„ä¸Šä¸‹æ–‡é™åˆ¶
   - å¹³è¡¡å¤„ç†æ•ˆç‡å’Œæ•ˆæœè´¨é‡
   - æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´ç­–ç•¥

3. **ä¸¤ç§åˆ†å—ç­–ç•¥å„é€‚ç”¨ä»€ä¹ˆåœºæ™¯ï¼Ÿ**
   - Tokenåˆ†å—ï¼šä¸¥æ ¼å¤§å°æ§åˆ¶ï¼Œæ‰¹é‡å¤„ç†
   - åˆ†éš”ç¬¦åˆ†å—ï¼šè¯­ä¹‰å®Œæ•´æ€§è¦æ±‚é«˜çš„åœºæ™¯

## âœ… æ£€æŸ¥ç‚¹

å®Œæˆè¿™ä¸€æ­¥åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- [ ] ç†è§£ä¸¤ç§åˆ†å—ç­–ç•¥çš„åŸç†å’Œå·®å¼‚
- [ ] æŒæ¡tiktokençš„ä½¿ç”¨æ–¹æ³•
- [ ] äº†è§£é‡å æœºåˆ¶çš„ä½œç”¨
- [ ] èƒ½å¤Ÿæ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„åˆ†å—å‚æ•°
- [ ] ç†è§£åˆ†å—è´¨é‡å¯¹åç»­å¤„ç†çš„å½±å“

## ğŸš€ ä¸‹ä¸€æ­¥é¢„å‘Š

**ç¬¬ä¸‰æ­¥ï¼šå®ä½“æå–ç³»ç»Ÿ**
- ä½¿ç”¨LLMä»æ–‡æœ¬åˆ†å—ä¸­æå–å®ä½“å’Œå…³ç³»
- å®ç°æç¤ºè¯å·¥ç¨‹å’Œè¾“å‡ºè§£æ
- æ„å»ºå®ä½“-å…³ç³»æ•°æ®ç»“æ„
- ä¸ºçŸ¥è¯†å›¾è°±æ„å»ºåšå‡†å¤‡ 