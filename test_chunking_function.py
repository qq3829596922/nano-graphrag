#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• chunking_by_token_size å‡½æ•°çš„ç‹¬ç«‹æµ‹è¯•æ–‡ä»¶
"""

import tiktoken
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from nano_graphrag._op import chunking_by_token_size


def test_chunking_by_token_size():
    """
    æµ‹è¯• chunking_by_token_size å‡½æ•°çš„åŠŸèƒ½
    """
    print("å¼€å§‹æµ‹è¯• chunking_by_token_size å‡½æ•°...")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    # åˆ›å»ºtiktokenç¼–ç å™¨
    encoder = tiktoken.encoding_for_model("gpt-4o")
    
    # æµ‹è¯•æ–‡æ¡£å†…å®¹
    test_docs = [
        "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚å®ƒåŒ…å«äº†ä¸€äº›ä¸­æ–‡å†…å®¹ç”¨äºæµ‹è¯•åˆ†å—åŠŸèƒ½ã€‚æˆ‘ä»¬éœ€è¦ç¡®ä¿åˆ†å—ç®—æ³•èƒ½å¤Ÿæ­£ç¡®å¤„ç†ä¸åŒé•¿åº¦çš„æ–‡æœ¬ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿æŒé€‚å½“çš„é‡å ã€‚",
        "This is the second test document. It contains English content to test the chunking functionality. We need to ensure that the chunking algorithm can handle texts of different lengths and maintain proper overlap between chunks.",
        "çŸ­æ–‡æ¡£"
    ]
    
    # æ–‡æ¡£é”®
    doc_keys = ["doc_1", "doc_2", "doc_3"]
    
    # å°†æ–‡æ¡£è½¬æ¢ä¸ºtokenåˆ—è¡¨
    tokens_list = []
    for doc in test_docs:
        tokens = encoder.encode(doc)
        tokens_list.append(tokens)
        print(f"æ–‡æ¡£: '{doc[:30]}...' -> Tokenæ•°é‡: {len(tokens)}")
    
    print(f"\nåŸå§‹tokens_listé•¿åº¦: {len(tokens_list)}")
    print(f"æ–‡æ¡£é”®: {doc_keys}")
    
    # æµ‹è¯•å‚æ•°
    overlap_token_size = 20
    max_token_size = 50
    
    print(f"\næµ‹è¯•å‚æ•°:")
    print(f"é‡å tokenå¤§å°: {overlap_token_size}")
    print(f"æœ€å¤§tokenå¤§å°: {max_token_size}")
    
    # è°ƒç”¨å‡½æ•°è¿›è¡Œæµ‹è¯•
    result = chunking_by_token_size(
        tokens_list=tokens_list,
        doc_keys=doc_keys,
        tiktoken_model=encoder,
        overlap_token_size=overlap_token_size,
        max_token_size=max_token_size
    )
    
    # è¾“å‡ºç»“æœ
    print(f"\næµ‹è¯•ç»“æœ:")
    print(f"æ€»åˆ†å—æ•°é‡: {len(result)}")
    print("\nåˆ†å—è¯¦æƒ…:")
    for i, chunk in enumerate(result):
        print(f"åˆ†å— {i+1}:")
        print(f"  - æ–‡æ¡£ID: {chunk['full_doc_id']}")
        print(f"  - åˆ†å—ç´¢å¼•: {chunk['chunk_order_index']}")
        print(f"  - Tokenæ•°é‡: {chunk['tokens']}")
        print(f"  - å†…å®¹: '{chunk['content'][:50]}...'")
        print()
    
    # éªŒè¯æµ‹è¯•ç»“æœ
    print("éªŒè¯æµ‹è¯•ç»“æœ:")
    
    # æ£€æŸ¥è¿”å›å€¼ç»“æ„
    assert isinstance(result, list), "è¿”å›å€¼åº”è¯¥æ˜¯åˆ—è¡¨"
    assert len(result) > 0, "åº”è¯¥æœ‰åˆ†å—ç»“æœ"
    
    # æ£€æŸ¥æ¯ä¸ªåˆ†å—çš„ç»“æ„
    for chunk in result:
        assert isinstance(chunk, dict), "æ¯ä¸ªåˆ†å—åº”è¯¥æ˜¯å­—å…¸"
        assert "tokens" in chunk, "åˆ†å—åº”è¯¥åŒ…å«tokenså­—æ®µ"
        assert "content" in chunk, "åˆ†å—åº”è¯¥åŒ…å«contentå­—æ®µ"
        assert "chunk_order_index" in chunk, "åˆ†å—åº”è¯¥åŒ…å«chunk_order_indexå­—æ®µ"
        assert "full_doc_id" in chunk, "åˆ†å—åº”è¯¥åŒ…å«full_doc_idå­—æ®µ"
        assert chunk["full_doc_id"] in doc_keys, "æ–‡æ¡£IDåº”è¯¥åœ¨åŸå§‹æ–‡æ¡£é”®ä¸­"
        assert chunk["tokens"] <= max_token_size, "åˆ†å—tokenæ•°é‡ä¸åº”è¶…è¿‡æœ€å¤§é™åˆ¶"
    
    # æ£€æŸ¥æ–‡æ¡£è¦†ç›–æƒ…å†µ
    doc_ids_in_result = set(chunk["full_doc_id"] for chunk in result)
    assert doc_ids_in_result == set(doc_keys), "æ‰€æœ‰æ–‡æ¡£éƒ½åº”è¯¥è¢«å¤„ç†"
    
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print(f"âœ… æˆåŠŸå¤„ç†äº† {len(doc_keys)} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆäº† {len(result)} ä¸ªåˆ†å—")
    
    # è¯¦ç»†åˆ†æå‡½æ•°è¡Œä¸º
    print("\nğŸ” å‡½æ•°è¡Œä¸ºåˆ†æ:")
    
    # åˆ†ææ¯ä¸ªæ–‡æ¡£çš„åˆ†å—æƒ…å†µ
    for doc_key in doc_keys:
        doc_chunks = [chunk for chunk in result if chunk["full_doc_id"] == doc_key]
        print(f"\næ–‡æ¡£ {doc_key}:")
        print(f"  - åŸæ–‡æ¡£tokenæ•°: {len(tokens_list[doc_keys.index(doc_key)])}")
        print(f"  - ç”Ÿæˆåˆ†å—æ•°: {len(doc_chunks)}")
        
        for chunk in doc_chunks:
            print(f"    åˆ†å— {chunk['chunk_order_index']}: {chunk['tokens']} tokens")
    
    # åˆ†æé‡å æƒ…å†µ
    print(f"\nğŸ“Š é‡å æƒ…å†µåˆ†æ:")
    print(f"å½“æ–‡æ¡£é•¿åº¦è¶…è¿‡ {max_token_size} tokensæ—¶ï¼Œä¼šæŒ‰ç…§æ»‘åŠ¨çª—å£è¿›è¡Œåˆ†å—")
    print(f"æ¯ä¸ªåˆ†å—æœ€å¤§ {max_token_size} tokensï¼Œé‡å  {overlap_token_size} tokens")
    print(f"æ»‘åŠ¨æ­¥é•¿: {max_token_size - overlap_token_size} tokens")
    
    return result


if __name__ == "__main__":
    test_result = test_chunking_by_token_size() 