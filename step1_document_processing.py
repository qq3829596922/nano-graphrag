#!/usr/bin/env python3
"""
ç¬¬ä¸€æ­¥ï¼šæ–‡æ¡£å¤„ç†æ¨¡å—
ç›®æ ‡ï¼šå®ç°æ–‡æ¡£çš„è¾“å…¥ã€IDç”Ÿæˆã€å­˜å‚¨åˆ°JSON

æ‚¨è¦å®ç°çš„åŠŸèƒ½ï¼š
1. æ–‡æ¡£IDç”Ÿæˆ
2. æ–‡æ¡£å­˜å‚¨åˆ°JSONæ–‡ä»¶
3. æ–‡æ¡£åŠ è½½å’ŒæŸ¥çœ‹
4. é‡å¤æ–‡æ¡£æ£€æµ‹
"""

import json
import hashlib
from pathlib import Path
from typing import List, Dict
from datetime import datetime

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    def __init__(self, storage_dir: str = "./step1_storage"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        self.docs_file = self.storage_dir / "documents.json"
        
        # åŠ è½½å·²æœ‰æ–‡æ¡£
        self.documents = self._load_documents()
    
    def _load_documents(self) -> Dict:
        """åŠ è½½å·²å­˜å‚¨çš„æ–‡æ¡£"""
        if self.docs_file.exists():
            with open(self.docs_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_documents(self):
        """ä¿å­˜æ–‡æ¡£åˆ°JSONæ–‡ä»¶"""
        with open(self.docs_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def generate_doc_id(self, content: str) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£ID
        
        TODO: è¯·å®ç°è¿™ä¸ªæ–¹æ³•
        è¦æ±‚ï¼š
        1. ä½¿ç”¨MD5å“ˆå¸Œç®—æ³•
        2. æ ¼å¼ï¼šdoc-{hashå‰8ä½}
        3. ç¡®ä¿ç›¸åŒå†…å®¹ç”Ÿæˆç›¸åŒID
        
        æç¤ºï¼š
        - ä½¿ç”¨ hashlib.md5()
        - è®°å¾— encode('utf-8')
        - ä½¿ç”¨ .hexdigest()[:8]
        """
        # åœ¨è¿™é‡Œå®ç°æ‚¨çš„ä»£ç 
        pass
    
    def add_document(self, content: str, metadata: Dict = None) -> str:
        """
        æ·»åŠ æ–‡æ¡£
        
        TODO: è¯·å®ç°è¿™ä¸ªæ–¹æ³•
        è¦æ±‚ï¼š
        1. ç”Ÿæˆæ–‡æ¡£ID
        2. æ£€æŸ¥æ˜¯å¦é‡å¤ï¼ˆå¦‚æœIDå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼‰
        3. å­˜å‚¨æ–‡æ¡£ä¿¡æ¯
        4. ä¿å­˜åˆ°JSONæ–‡ä»¶
        
        è¿”å›ï¼šæ–‡æ¡£ID
        """
        doc_id = self.generate_doc_id(content)
        
        # æ£€æŸ¥é‡å¤
        if doc_id in self.documents:
            print(f"âš ï¸  æ–‡æ¡£å·²å­˜åœ¨: {doc_id}")
            return doc_id
        
        # TODO: åˆ›å»ºæ–‡æ¡£å¯¹è±¡å¹¶å­˜å‚¨
        # æ–‡æ¡£å¯¹è±¡åº”è¯¥åŒ…å«ï¼š
        # - content: æ–‡æ¡£å†…å®¹
        # - metadata: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        # - created_at: åˆ›å»ºæ—¶é—´
        # - doc_id: æ–‡æ¡£ID
        
        # TODO: ä¿å­˜åˆ°æ–‡ä»¶
        
        print(f"âœ… å·²æ·»åŠ æ–‡æ¡£: {doc_id}")
        return doc_id
    
    def get_document(self, doc_id: str) -> Dict:
        """è·å–æ–‡æ¡£"""
        return self.documents.get(doc_id)
    
    def list_documents(self):
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        print(f"ğŸ“š å…±æœ‰ {len(self.documents)} ä¸ªæ–‡æ¡£:")
        for doc_id, doc_data in self.documents.items():
            preview = doc_data['content'][:50] + "..." if len(doc_data['content']) > 50 else doc_data['content']
            created_at = doc_data.get('created_at', 'æœªçŸ¥')
            print(f"  ğŸ“„ {doc_id}: {preview} (åˆ›å»ºæ—¶é—´: {created_at})")
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.documents:
            print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: æš‚æ— æ–‡æ¡£")
            return
        
        total_docs = len(self.documents)
        total_chars = sum(len(doc['content']) for doc in self.documents.values())
        avg_length = total_chars // total_docs
        
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ–‡æ¡£æ•°é‡: {total_docs}")
        print(f"  æ€»å­—ç¬¦æ•°: {total_chars}")
        print(f"  å¹³å‡é•¿åº¦: {avg_length} å­—ç¬¦")

def test_document_processor():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨")
    print("=" * 40)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = DocumentProcessor()
    
    # æµ‹è¯•æ–‡æ¡£
    test_docs = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨ä¸è¢«æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"  # é‡å¤æ–‡æ¡£
    ]
    
    # æ·»åŠ æ–‡æ¡£
    print("\nğŸ“ æ·»åŠ æ–‡æ¡£:")
    for i, doc in enumerate(test_docs):
        doc_id = processor.add_document(doc, {"source": f"test_doc_{i}"})
    
    print("\nğŸ“‹ æ–‡æ¡£åˆ—è¡¨:")
    processor.list_documents()
    
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    processor.get_stats()
    
    print("\nğŸ” æµ‹è¯•æ–‡æ¡£æ£€ç´¢:")
    # è·å–ç¬¬ä¸€ä¸ªæ–‡æ¡£
    all_docs = list(processor.documents.keys())
    if all_docs:
        first_doc = processor.get_document(all_docs[0])
        print(f"æ–‡æ¡£è¯¦æƒ…: {first_doc}")

if __name__ == "__main__":
    test_document_processor()
    
    print("\n" + "="*50)
    print("ğŸ¯ å®Œæˆç¬¬ä¸€æ­¥åï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š")
    print("1. ä¸ºä»€ä¹ˆè¦ä½¿ç”¨å“ˆå¸Œç”ŸæˆIDè€Œä¸æ˜¯è‡ªå¢æ•°å­—ï¼Ÿ")
    print("2. JSONå­˜å‚¨çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("3. å¦‚ä½•å¤„ç†æ–‡æ¡£æ›´æ–°çš„æƒ…å†µï¼Ÿ")
    print("4. æ‚¨è§‰å¾—æ–‡æ¡£å¯¹è±¡è¿˜åº”è¯¥åŒ…å«å“ªäº›å­—æ®µï¼Ÿ")
    print("å®Œæˆåï¼Œæˆ‘ä»¬å°†è¿›å…¥ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬åˆ†å—") 