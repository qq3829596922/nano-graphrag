"""
GraphRAGå­¦ä¹ ç³»åˆ— - ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬å¤„ç†ç³»ç»Ÿ
==================================

è¿™ä¸€æ­¥æˆ‘ä»¬å­¦ä¹ GraphRAGä¸­çš„æ–‡æœ¬åˆ†å—ï¼ˆText Chunkingï¼‰åŠŸèƒ½ã€‚
åˆ†å—æ˜¯å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆé€‚åˆLLMå¤„ç†çš„å°æ®µè½ï¼Œè¿™æ˜¯æ„å»ºçŸ¥è¯†å›¾è°±çš„ç¬¬äºŒä¸ªå…³é”®æ­¥éª¤ã€‚

å­¦ä¹ é‡ç‚¹ï¼š
1. åŸºäºTokenæ•°é‡çš„åˆ†å—ç­–ç•¥
2. åŸºäºåˆ†éš”ç¬¦çš„æ™ºèƒ½åˆ†å—
3. é‡å æœºåˆ¶é˜²æ­¢ä¿¡æ¯ä¸¢å¤±
4. æ‰¹é‡å¤„ç†ä¼˜åŒ–æ€§èƒ½
5. ä¸å­˜å‚¨ç³»ç»Ÿçš„é›†æˆ

å‰ç½®ä¾èµ–ï¼šstep1_document_processing.py
"""

import os
import json
import hashlib
import asyncio
import tiktoken
from pathlib import Path
from typing import List, Dict, Optional, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime


# ====================== åˆ†å—ç»“æœæ•°æ®ç»“æ„ ======================

@dataclass
class ChunkResult:
    """æ–‡æ¡£åˆ†å—çš„ç»“æœæ•°æ®ç»“æ„"""
    content: str          # åˆ†å—çš„æ–‡æœ¬å†…å®¹
    token_count: int      # Tokenæ•°é‡
    chunk_index: int      # åœ¨åŸæ–‡æ¡£ä¸­çš„åˆ†å—é¡ºåº
    doc_id: str          # åŸå§‹æ–‡æ¡£ID
    chunk_id: str        # åˆ†å—çš„å”¯ä¸€IDï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆä¸GraphRAGå…¼å®¹ï¼‰"""
        return {
            "tokens": self.token_count,
            "content": self.content,
            "chunk_order_index": self.chunk_index,
            "full_doc_id": self.doc_id,
        }


# ====================== åˆ†éš”ç¬¦åˆ†å‰²å™¨ ======================

class SeparatorSplitter:
    """
    åŸºäºåˆ†éš”ç¬¦çš„æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨
    
    è¿™ä¸ªç±»å®ç°äº†GraphRAGä¸­çš„SeparatorSplitteré€»è¾‘ï¼Œ
    å¯ä»¥æ ¹æ®è¯­ä¹‰è¾¹ç•Œï¼ˆæ®µè½ã€å¥å­ã€è¯è¯­ï¼‰è¿›è¡Œåˆ†å‰²ï¼Œ
    å¹¶è‡ªåŠ¨åˆå¹¶å°ç‰‡æ®µåˆ°åˆé€‚çš„å¤§å°ã€‚
    """
    
    def __init__(
        self,
        separators: Optional[List[List[int]]] = None,  # åˆ†éš”ç¬¦çš„tokenè¡¨ç¤º
        keep_separator: Union[bool, str] = "end",      # ä¿ç•™åˆ†éš”ç¬¦ä½ç½®
        chunk_size: int = 1200,                        # ç›®æ ‡åˆ†å—å¤§å°
        chunk_overlap: int = 100,                      # é‡å tokenæ•°é‡
        length_function: Callable = len,               # é•¿åº¦è®¡ç®—å‡½æ•°
    ):
        self._separators = separators or []
        self._keep_separator = keep_separator
        self._chunk_size = chunk_size
        self._chunk_overlap = chunk_overlap
        self._length_function = length_function
    
    def split_tokens(self, tokens: List[int]) -> List[List[int]]:
        """
        æ ¸å¿ƒåˆ†å‰²æ–¹æ³•ï¼šå°†tokenåˆ—è¡¨åˆ†å‰²æˆåˆé€‚å¤§å°çš„åˆ†å—
        
        æµç¨‹ï¼š
        1. æŒ‰ç…§åˆ†éš”ç¬¦æ‰¾åˆ°åˆ‡åˆ†ç‚¹
        2. åˆå¹¶å°ç‰‡æ®µåˆ°åˆç†å¤§å°
        3. æ·»åŠ é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
        """
        # ç¬¬ä¸€æ­¥ï¼šæŒ‰åˆ†éš”ç¬¦åˆ†å‰²
        splits = self._split_tokens_with_separators(tokens)
        
        # ç¬¬äºŒæ­¥ï¼šåˆå¹¶å¹¶ä¼˜åŒ–åˆ†å—å¤§å°
        return self._merge_splits(splits)
    
    def _split_tokens_with_separators(self, tokens: List[int]) -> List[List[int]]:
        """ä½¿ç”¨åˆ†éš”ç¬¦æ‰¾åˆ°æ‰€æœ‰åˆ‡åˆ†ç‚¹"""
        splits = []
        current_split = []
        i = 0
        
        while i < len(tokens):
            separator_found = False
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•åˆ†éš”ç¬¦
            for separator in self._separators:
                if tokens[i:i+len(separator)] == separator:
                    # å¤„ç†åˆ†éš”ç¬¦çš„ä¿ç•™ç­–ç•¥
                    if self._keep_separator in [True, "end"]:
                        current_split.extend(separator)
                    
                    if current_split:
                        splits.append(current_split)
                        current_split = []
                    
                    if self._keep_separator == "start":
                        current_split.extend(separator)
                    
                    i += len(separator)
                    separator_found = True
                    break
            
            if not separator_found:
                current_split.append(tokens[i])
                i += 1
        
        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å‰²
        if current_split:
            splits.append(current_split)
        
        return [s for s in splits if s]  # è¿‡æ»¤ç©ºåˆ†å‰²
    
    def _merge_splits(self, splits: List[List[int]]) -> List[List[int]]:
        """æ™ºèƒ½åˆå¹¶å°åˆ†å‰²åˆ°åˆé€‚çš„åˆ†å—å¤§å°"""
        if not splits:
            return []
        
        merged_splits = []
        current_chunk = []
        
        for split in splits:
            if not current_chunk:
                current_chunk = split
            elif (self._length_function(current_chunk) + 
                  self._length_function(split) <= self._chunk_size):
                # å¯ä»¥åˆå¹¶
                current_chunk.extend(split)
            else:
                # å½“å‰åˆ†å—å·²æ»¡ï¼Œå¼€å§‹æ–°åˆ†å—
                merged_splits.append(current_chunk)
                current_chunk = split
        
        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk:
            merged_splits.append(current_chunk)
        
        # å¤„ç†è¿‡å¤§çš„åˆ†å—
        if (len(merged_splits) == 1 and 
            self._length_function(merged_splits[0]) > self._chunk_size):
            return self._split_large_chunk(merged_splits[0])
        
        # æ·»åŠ é‡å æœºåˆ¶
        if self._chunk_overlap > 0:
            return self._add_overlap(merged_splits)
        
        return merged_splits
    
    def _split_large_chunk(self, chunk: List[int]) -> List[List[int]]:
        """å°†è¿‡å¤§çš„åˆ†å—è¿›ä¸€æ­¥åˆ‡åˆ†"""
        result = []
        step_size = self._chunk_size - self._chunk_overlap
        
        for i in range(0, len(chunk), step_size):
            new_chunk = chunk[i:i + self._chunk_size]
            if len(new_chunk) > self._chunk_overlap:  # é¿å…å¤ªå°çš„åˆ†å—
                result.append(new_chunk)
        
        return result
    
    def _add_overlap(self, chunks: List[List[int]]) -> List[List[int]]:
        """ä¸ºåˆ†å—æ·»åŠ é‡å åŒºåŸŸä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§"""
        if len(chunks) <= 1:
            return chunks
        
        result = [chunks[0]]  # ç¬¬ä¸€ä¸ªåˆ†å—ä¸éœ€è¦é‡å 
        
        for i in range(1, len(chunks)):
            # ä»å‰ä¸€ä¸ªåˆ†å—å–é‡å éƒ¨åˆ†
            prev_chunk = chunks[i-1]
            current_chunk = chunks[i]
            
            overlap_tokens = prev_chunk[-self._chunk_overlap:]
            new_chunk = overlap_tokens + current_chunk
            
            # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§å¤§å°
            if self._length_function(new_chunk) > self._chunk_size:
                new_chunk = new_chunk[:self._chunk_size]
            
            result.append(new_chunk)
        
        return result


# ====================== æ–‡æœ¬å¤„ç†å™¨ä¸»ç±» ======================

class TextProcessor:
    """
    GraphRAGçš„æ–‡æœ¬å¤„ç†æ ¸å¿ƒç±»
    
    è´Ÿè´£å°†æ–‡æ¡£è½¬æ¢ä¸ºé€‚åˆåç»­å¤„ç†çš„æ–‡æœ¬åˆ†å—ï¼Œ
    æ”¯æŒå¤šç§åˆ†å—ç­–ç•¥å’Œæ‰¹é‡å¤„ç†ä¼˜åŒ–ã€‚
    """
    
    def __init__(
        self,
        model_name: str = "gpt-4o",
        default_chunk_size: int = 1200,
        default_overlap_size: int = 100,
        enable_tiktoken_cache: bool = True,
    ):
        self.model_name = model_name
        self.default_chunk_size = default_chunk_size
        self.default_overlap_size = default_overlap_size
        
        # åˆå§‹åŒ–tiktokenç¼–ç å™¨ï¼ˆç”¨äºç²¾ç¡®è®¡ç®—tokenæ•°é‡ï¼‰
        self.encoder = tiktoken.encoding_for_model(model_name)
        
        # é»˜è®¤çš„æ™ºèƒ½åˆ†éš”ç¬¦ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åˆ—ï¼‰
        self.default_separators = [
            "\n\n",      # æ®µè½åˆ†éš”ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            "\n",        # è¡Œåˆ†éš”
            "ã€‚",        # ä¸­æ–‡å¥å·
            "ï¼",        # å…¨è§’å¥å·
            ".",         # è‹±æ–‡å¥å·
            "ï¼",        # ä¸­æ–‡æ„Ÿå¹å·
            "!",         # è‹±æ–‡æ„Ÿå¹å·
            "ï¼Ÿ",        # ä¸­æ–‡é—®å·
            "?",         # è‹±æ–‡é—®å·
            "ï¼›",        # ä¸­æ–‡åˆ†å·
            ";",         # è‹±æ–‡åˆ†å·
            "ï¼Œ",        # ä¸­æ–‡é€—å·
            ",",         # è‹±æ–‡é€—å·
            " ",         # ç©ºæ ¼ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
        ]
        
        print(f"âœ… æ–‡æœ¬å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ¨¡å‹: {model_name}")
        print(f"   - é»˜è®¤åˆ†å—å¤§å°: {default_chunk_size} tokens")
        print(f"   - é»˜è®¤é‡å å¤§å°: {default_overlap_size} tokens")
    
    def compute_chunk_id(self, content: str, prefix: str = "chunk-") -> str:
        """ä¸ºåˆ†å—å†…å®¹ç”Ÿæˆå”¯ä¸€ID"""
        return prefix + hashlib.md5(content.encode()).hexdigest()
    
    def chunking_by_token_size(
        self,
        documents: Dict[str, str],  # {doc_id: content}
        max_token_size: Optional[int] = None,
        overlap_token_size: Optional[int] = None,
    ) -> List[ChunkResult]:
        """
        åŸºäºTokenæ•°é‡çš„æ–‡æ¡£åˆ†å—æ–¹æ³•
        
        è¿™æ˜¯GraphRAGä¸­çš„ç»å…¸åˆ†å—ç­–ç•¥ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶ï¼š
        - ç²¾ç¡®æ§åˆ¶æ¯ä¸ªåˆ†å—çš„tokenæ•°é‡
        - è®¾ç½®é‡å åŒºåŸŸé˜²æ­¢ä¿¡æ¯ä¸¢å¤±
        - ä½¿ç”¨æ‰¹é‡ç¼–ç ä¼˜åŒ–æ€§èƒ½
        
        å‚æ•°ï¼š
            documents: è¦å¤„ç†çš„æ–‡æ¡£å­—å…¸
            max_token_size: æœ€å¤§åˆ†å—tokenæ•°
            overlap_token_size: é‡å tokenæ•°
        
        è¿”å›ï¼š
            åˆ†å—ç»“æœåˆ—è¡¨
        """
        if max_token_size is None:
            max_token_size = self.default_chunk_size
        if overlap_token_size is None:
            overlap_token_size = self.default_overlap_size
        
        print(f"ğŸ”§ å¼€å§‹åŸºäºTokenæ•°é‡çš„åˆ†å—...")
        print(f"   - åˆ†å—å¤§å°: {max_token_size} tokens")
        print(f"   - é‡å å¤§å°: {overlap_token_size} tokens")
        print(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
        
        results = []
        doc_ids = list(documents.keys())
        contents = list(documents.values())
        
        # æ‰¹é‡ç¼–ç æ‰€æœ‰æ–‡æ¡£ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        print("   - æ­£åœ¨æ‰¹é‡ç¼–ç æ–‡æ¡£...")
        tokens_list = self.encoder.encode_batch(contents, num_threads=16)
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        for doc_index, tokens in enumerate(tokens_list):
            doc_id = doc_ids[doc_index]
            print(f"   - å¤„ç†æ–‡æ¡£ {doc_id}: {len(tokens)} tokens")
            
            chunk_tokens_list = []
            token_lengths = []
            
            # æ»‘åŠ¨çª—å£åˆ†å—
            step_size = max_token_size - overlap_token_size
            for start in range(0, len(tokens), step_size):
                end = start + max_token_size
                chunk_tokens = tokens[start:end]
                chunk_tokens_list.append(chunk_tokens)
                token_lengths.append(len(chunk_tokens))
            
            # æ‰¹é‡è§£ç tokenä¸ºæ–‡æœ¬
            chunk_contents = self.encoder.decode_batch(chunk_tokens_list)
            
            # æ„å»ºåˆ†å—ç»“æœ
            for i, content in enumerate(chunk_contents):
                content = content.strip()
                if content:  # åªä¿ç•™éç©ºå†…å®¹
                    chunk_result = ChunkResult(
                        content=content,
                        token_count=token_lengths[i],
                        chunk_index=i,
                        doc_id=doc_id,
                        chunk_id=self.compute_chunk_id(content)
                    )
                    results.append(chunk_result)
            
            print(f"     â†’ ç”Ÿæˆ {len(chunk_contents)} ä¸ªåˆ†å—")
        
        print(f"âœ… Tokenåˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} ä¸ªåˆ†å—")
        return results
    
    def chunking_by_separators(
        self,
        documents: Dict[str, str],
        max_token_size: Optional[int] = None,
        overlap_token_size: Optional[int] = None,
        separators: Optional[List[str]] = None,
    ) -> List[ChunkResult]:
        """
        åŸºäºåˆ†éš”ç¬¦çš„æ™ºèƒ½æ–‡æ¡£åˆ†å—æ–¹æ³•
        
        è¿™ç§æ–¹æ³•æ›´å…³æ³¨è¯­ä¹‰å®Œæ•´æ€§ï¼š
        - æŒ‰ç…§è¯­ä¹‰è¾¹ç•Œï¼ˆæ®µè½ã€å¥å­ï¼‰è¿›è¡Œåˆ†å‰²
        - æ™ºèƒ½åˆå¹¶å°ç‰‡æ®µåˆ°åˆé€‚å¤§å°
        - ä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§
        
        å‚æ•°ï¼š
            documents: è¦å¤„ç†çš„æ–‡æ¡£å­—å…¸
            max_token_size: æœ€å¤§åˆ†å—tokenæ•°
            overlap_token_size: é‡å tokenæ•°
            separators: è‡ªå®šä¹‰åˆ†éš”ç¬¦åˆ—è¡¨
        
        è¿”å›ï¼š
            åˆ†å—ç»“æœåˆ—è¡¨
        """
        if max_token_size is None:
            max_token_size = self.default_chunk_size
        if overlap_token_size is None:
            overlap_token_size = self.default_overlap_size
        if separators is None:
            separators = self.default_separators
        
        print(f"ğŸ”§ å¼€å§‹åŸºäºåˆ†éš”ç¬¦çš„æ™ºèƒ½åˆ†å—...")
        print(f"   - åˆ†å—å¤§å°: {max_token_size} tokens")
        print(f"   - é‡å å¤§å°: {overlap_token_size} tokens")
        print(f"   - åˆ†éš”ç¬¦æ•°é‡: {len(separators)}")
        print(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
        
        # å°†åˆ†éš”ç¬¦ç¼–ç ä¸ºtoken
        separator_tokens = [self.encoder.encode(s) for s in separators]
        
        # åˆ›å»ºåˆ†éš”ç¬¦åˆ†å‰²å™¨
        splitter = SeparatorSplitter(
            separators=separator_tokens,
            chunk_size=max_token_size,
            chunk_overlap=overlap_token_size,
        )
        
        results = []
        doc_ids = list(documents.keys())
        contents = list(documents.values())
        
        # æ‰¹é‡ç¼–ç 
        print("   - æ­£åœ¨æ‰¹é‡ç¼–ç æ–‡æ¡£...")
        tokens_list = self.encoder.encode_batch(contents, num_threads=16)
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        for doc_index, tokens in enumerate(tokens_list):
            doc_id = doc_ids[doc_index]
            print(f"   - å¤„ç†æ–‡æ¡£ {doc_id}: {len(tokens)} tokens")
            
            # ä½¿ç”¨åˆ†å‰²å™¨åˆ†å—
            chunk_tokens_list = splitter.split_tokens(tokens)
            token_lengths = [len(chunk) for chunk in chunk_tokens_list]
            
            # è§£ç ä¸ºæ–‡æœ¬
            chunk_contents = self.encoder.decode_batch(chunk_tokens_list)
            
            # æ„å»ºç»“æœ
            for i, content in enumerate(chunk_contents):
                content = content.strip()
                if content:
                    chunk_result = ChunkResult(
                        content=content,
                        token_count=token_lengths[i],
                        chunk_index=i,
                        doc_id=doc_id,
                        chunk_id=self.compute_chunk_id(content)
                    )
                    results.append(chunk_result)
            
            print(f"     â†’ ç”Ÿæˆ {len(chunk_contents)} ä¸ªåˆ†å—")
        
        print(f"âœ… åˆ†éš”ç¬¦åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} ä¸ªåˆ†å—")
        return results
    
    def export_chunks_to_graphrag_format(
        self, 
        chunks: List[ChunkResult]
    ) -> Dict[str, Dict]:
        """
        å°†åˆ†å—ç»“æœå¯¼å‡ºä¸ºGraphRAGå…¼å®¹çš„æ ¼å¼
        
        æ ¼å¼ï¼š{chunk_id: chunk_data}
        """
        result = {}
        for chunk in chunks:
            result[chunk.chunk_id] = chunk.to_dict()
        return result
    
    async def save_chunks_to_storage(
        self,
        chunks: List[ChunkResult],
        storage_dir: str = "step2_chunks_storage"
    ):
        """å°†åˆ†å—ç»“æœä¿å­˜åˆ°å­˜å‚¨ç³»ç»Ÿï¼ˆç®€åŒ–ç‰ˆå­˜å‚¨ï¼‰"""
        # åˆ›å»ºå­˜å‚¨ç›®å½•
        storage_path = Path(storage_dir)
        storage_path.mkdir(exist_ok=True)
        
        # è½¬æ¢ä¸ºGraphRAGæ ¼å¼
        chunk_data = self.export_chunks_to_graphrag_format(chunks)
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        chunks_file = storage_path / "text_chunks.json"
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²ä¿å­˜ {len(chunks)} ä¸ªåˆ†å—åˆ°å­˜å‚¨ç³»ç»Ÿ")
        print(f"   - å­˜å‚¨ä½ç½®: {chunks_file}")
        
        return chunk_data


# ====================== æ¼”ç¤ºå’Œæµ‹è¯• ======================

async def demo_text_processing():
    """æ¼”ç¤ºæ–‡æœ¬å¤„ç†åŠŸèƒ½"""
    print("=" * 60)
    print("GraphRAGå­¦ä¹ ç³»åˆ— - ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬å¤„ç†ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # å‡†å¤‡æµ‹è¯•æ–‡æ¡£ï¼ˆæ›´ä¸°å¯Œçš„å†…å®¹ï¼‰
    test_documents = {
        "doc-ai-overview": """
        äººå·¥æ™ºèƒ½æ¦‚è¿°
        
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œè‡´åŠ›äºç ”ç©¶ã€å¼€å‘ç”¨äºæ¨¡æ‹Ÿã€å»¶ä¼¸å’Œæ‰©å±•äººçš„æ™ºèƒ½çš„ç†è®ºã€æ–¹æ³•ã€æŠ€æœ¯åŠåº”ç”¨ç³»ç»Ÿã€‚
        
        AIçš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚1956å¹´ï¼Œçº¦ç¿°Â·éº¦å¡é”¡é¦–æ¬¡æå‡ºäº†"äººå·¥æ™ºèƒ½"è¿™ä¸€æœ¯è¯­ã€‚ä»é‚£æ—¶èµ·ï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ï¼ŒåŒ…æ‹¬ä¸¤æ¬¡AIå¯’å†¬æœŸã€‚
        
        æœºå™¨å­¦ä¹ çš„å…´èµ·
        
        æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼ŒMLï¼‰æ˜¯AIçš„ä¸€ä¸ªæ ¸å¿ƒåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ ã€‚æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿè¯†åˆ«æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹ã€‚
        
        æ·±åº¦å­¦ä¹ é©å‘½
        
        æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼ŒDLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼ŒåŸºäºäººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
        
        ç°ä»£AIåº”ç”¨
        
        ä»Šå¤©çš„AIæŠ€æœ¯å·²ç»å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼šæœç´¢å¼•æ“ã€æ¨èç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰ã€‚ChatGPTç­‰å¤§è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œæ›´æ˜¯å°†AIæŠ€æœ¯æ¨å‘äº†æ–°çš„é«˜åº¦ã€‚
        """,
        
        "doc-graphrag-intro": """
        GraphRAGæŠ€æœ¯è¯¦è§£
        
        GraphRAGï¼ˆGraph Retrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§åˆ›æ–°çš„RAGæ¶æ„ï¼Œå®ƒç»“åˆäº†çŸ¥è¯†å›¾è°±å’Œæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ã€‚
        
        ä¼ ç»ŸRAGçš„å±€é™æ€§
        
        ä¼ ç»Ÿçš„RAGç³»ç»Ÿä¸»è¦ä¾èµ–å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶å­˜åœ¨å±€é™ï¼š
        1. éš¾ä»¥æ•è·å®ä½“é—´çš„å¤æ‚å…³ç³»
        2. ç¼ºä¹å…¨å±€æ€§çš„æ¨ç†èƒ½åŠ›
        3. å¯¹äºéœ€è¦å¤šè·³æ¨ç†çš„é—®é¢˜æ•ˆæœä¸ä½³
        
        GraphRAGçš„åˆ›æ–°
        
        GraphRAGé€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±æ¥è§£å†³è¿™äº›é—®é¢˜ï¼š
        - å®ä½“æå–ï¼šä»æ–‡æ¡£ä¸­è¯†åˆ«å‘½åå®ä½“
        - å…³ç³»æå–ï¼šæŒ–æ˜å®ä½“é—´çš„è¯­ä¹‰å…³ç³»
        - å›¾è°±æ„å»ºï¼šå°†å®ä½“å’Œå…³ç³»ç»„ç»‡æˆå›¾ç»“æ„
        - ç¤¾åŒºæ£€æµ‹ï¼šå‘ç°å®ä½“é›†ç¾¤å’Œä¸»é¢˜
        
        æŸ¥è¯¢å¤„ç†æœºåˆ¶
        
        GraphRAGæ”¯æŒä¸‰ç§æŸ¥è¯¢æ¨¡å¼ï¼š
        1. LocalæŸ¥è¯¢ï¼šåŸºäºå±€éƒ¨å­å›¾çš„è¯¦ç»†åˆ†æ
        2. GlobalæŸ¥è¯¢ï¼šåŸºäºç¤¾åŒºæ‘˜è¦çš„å…¨å±€ç†è§£
        3. NaiveæŸ¥è¯¢ï¼šä¼ ç»Ÿçš„å‘é‡æ£€ç´¢æ–¹å¼
        
        æŠ€æœ¯ä¼˜åŠ¿
        
        ç›¸æ¯”ä¼ ç»ŸRAGï¼ŒGraphRAGå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼šæ›´å¥½çš„æ¨ç†èƒ½åŠ›ã€æ›´å¼ºçš„å¯è§£é‡Šæ€§ã€æ›´å‡†ç¡®çš„ç­”æ¡ˆç”Ÿæˆã€‚
        """
    }
    
    # åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    processor = TextProcessor(
        default_chunk_size=400,   # ä½¿ç”¨è¾ƒå°çš„åˆ†å—ä¾¿äºæ¼”ç¤º
        default_overlap_size=50
    )
    
    print("\n" + "="*50)
    print("ğŸ“ æµ‹è¯•1: åŸºäºTokenæ•°é‡çš„åˆ†å—")
    print("="*50)
    
    token_chunks = processor.chunking_by_token_size(test_documents)
    
    print(f"\nğŸ“Š åˆ†å—ç»Ÿè®¡:")
    for i, chunk in enumerate(token_chunks):
        print(f"åˆ†å— {i+1}:")
        print(f"  - æ¥æºæ–‡æ¡£: {chunk.doc_id}")
        print(f"  - Tokenæ•°é‡: {chunk.token_count}")
        print(f"  - åˆ†å—é¡ºåº: {chunk.chunk_index}")
        print(f"  - åˆ†å—ID: {chunk.chunk_id}")
        print(f"  - å†…å®¹é¢„è§ˆ: {chunk.content[:80]}...")
        print()
    
    print("\n" + "="*50)
    print("ğŸ“ æµ‹è¯•2: åŸºäºåˆ†éš”ç¬¦çš„æ™ºèƒ½åˆ†å—")
    print("="*50)
    
    separator_chunks = processor.chunking_by_separators(test_documents)
    
    print(f"\nğŸ“Š åˆ†å—ç»Ÿè®¡:")
    for i, chunk in enumerate(separator_chunks):
        print(f"åˆ†å— {i+1}:")
        print(f"  - æ¥æºæ–‡æ¡£: {chunk.doc_id}")
        print(f"  - Tokenæ•°é‡: {chunk.token_count}")
        print(f"  - åˆ†å—é¡ºåº: {chunk.chunk_index}")
        print(f"  - åˆ†å—ID: {chunk.chunk_id}")
        print(f"  - å†…å®¹é¢„è§ˆ: {chunk.content[:80]}...")
        print()
    
    print("\n" + "="*50)
    print("ğŸ“ æµ‹è¯•3: ä¸å­˜å‚¨ç³»ç»Ÿé›†æˆ")
    print("="*50)
    
    # ä¿å­˜åˆ†å—åˆ°å­˜å‚¨ç³»ç»Ÿ
    chunk_data = await processor.save_chunks_to_storage(token_chunks)
    
    print(f"\nğŸ“Š å­˜å‚¨æ ¼å¼é¢„è§ˆ:")
    for chunk_id, chunk_info in list(chunk_data.items())[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
        print(f"åˆ†å—ID: {chunk_id}")
        print(f"æ•°æ®ç»“æ„: {json.dumps(chunk_info, ensure_ascii=False, indent=2)}")
        print()
    
    print("\n" + "="*50)
    print("ğŸ“ æµ‹è¯•4: æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("="*50)
    
    import time
    
    # Tokenåˆ†å—æ€§èƒ½æµ‹è¯•
    start_time = time.time()
    token_chunks = processor.chunking_by_token_size(test_documents)
    token_time = time.time() - start_time
    
    # åˆ†éš”ç¬¦åˆ†å—æ€§èƒ½æµ‹è¯•
    start_time = time.time()
    separator_chunks = processor.chunking_by_separators(test_documents)
    separator_time = time.time() - start_time
    
    print(f"âš¡ æ€§èƒ½å¯¹æ¯”:")
    print(f"  - Tokenåˆ†å—: {token_time:.4f}s, ç”Ÿæˆ {len(token_chunks)} ä¸ªåˆ†å—")
    print(f"  - åˆ†éš”ç¬¦åˆ†å—: {separator_time:.4f}s, ç”Ÿæˆ {len(separator_chunks)} ä¸ªåˆ†å—")
    print(f"  - æ€§èƒ½æ¯”å€¼: {separator_time/token_time:.2f}x")
    
    print("\n" + "="*50)
    print("âœ… æ–‡æœ¬å¤„ç†ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("="*50)
    print("\nä¸‹ä¸€æ­¥å­¦ä¹ :")
    print("- step3_entity_extraction.py: å®ä½“æå–ç³»ç»Ÿ")
    print("- ä»æ–‡æœ¬åˆ†å—ä¸­è¯†åˆ«å®ä½“å’Œå…³ç³»")
    print("- ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½è§£æ")


def analyze_chunking_strategies():
    """åˆ†æä¸åŒåˆ†å—ç­–ç•¥çš„ç‰¹ç‚¹"""
    print("\n" + "="*60)
    print("ğŸ“‹ åˆ†å—ç­–ç•¥å¯¹æ¯”åˆ†æ")
    print("="*60)
    
    comparison_data = [
        ["ç‰¹æ€§", "Tokenåˆ†å—", "åˆ†éš”ç¬¦åˆ†å—"],
        ["ç²¾ç¡®æ€§", "ç²¾ç¡®æ§åˆ¶å¤§å°", "è¯­ä¹‰è¾¹ç•Œä¼˜å…ˆ"],
        ["è¯­ä¹‰å®Œæ•´æ€§", "å¯èƒ½æˆªæ–­å¥å­", "ä¿æŒè¯­ä¹‰å®Œæ•´"],
        ["å¤„ç†é€Ÿåº¦", "å¿«", "ç¨æ…¢ï¼ˆéœ€è¦åˆ†éš”ç¬¦åŒ¹é…ï¼‰"],
        ["å†…å­˜ä½¿ç”¨", "ä½", "ä¸­ç­‰"],
        ["é€‚ç”¨åœºæ™¯", "ä¸¥æ ¼å¤§å°è¦æ±‚", "è¯­ä¹‰è´¨é‡è¦æ±‚"],
        ["é‡å å¤„ç†", "ç®€å•æ»‘åŠ¨çª—å£", "æ™ºèƒ½è¾¹ç•Œå¯¹é½"],
        ["å¯å®šåˆ¶æ€§", "å‚æ•°ç®€å•", "å¯å®šåˆ¶åˆ†éš”ç¬¦è§„åˆ™"],
    ]
    
    # æ‰“å°è¡¨æ ¼
    for row in comparison_data:
        print(f"| {row[0]:<12} | {row[1]:<15} | {row[2]:<20} |")
        if row[0] == "ç‰¹æ€§":
            print("|" + "-"*14 + "|" + "-"*17 + "|" + "-"*22 + "|")
    
    print(f"\nğŸ’¡ é€‰æ‹©å»ºè®®:")
    print(f"  - å¯¹äºä¸¥æ ¼æ§åˆ¶åˆ†å—å¤§å°çš„åœºæ™¯ï¼Œé€‰æ‹©Tokenåˆ†å—")
    print(f"  - å¯¹äºæ³¨é‡è¯­ä¹‰å®Œæ•´æ€§çš„åœºæ™¯ï¼Œé€‰æ‹©åˆ†éš”ç¬¦åˆ†å—")
    print(f"  - å¯¹äºæ··åˆéœ€æ±‚ï¼Œå¯ä»¥ç»„åˆä½¿ç”¨ä¸¤ç§ç­–ç•¥")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_text_processing())
    
    # åˆ†æåˆ†å—ç­–ç•¥
    analyze_chunking_strategies()