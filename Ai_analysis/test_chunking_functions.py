"""
æµ‹è¯•æ–‡æœ¬åˆ†å—å‡½æ•°çš„ç›´æ¥è°ƒç”¨
æ¼”ç¤º nano_graphrag ä¸­çš„åˆ†å—å‡½æ•°å¦‚ä½•å·¥ä½œ
"""

import json
import tiktoken
from nano_graphrag._op import chunking_by_token_size, chunking_by_seperators, get_chunks
from nano_graphrag._utils import compute_mdhash_id

def test_chunking_functions():
    """æµ‹è¯•å’Œæ¼”ç¤ºåˆ†å—å‡½æ•°çš„å·¥ä½œåŸç†"""
    
    print("=== æ–‡æœ¬åˆ†å—å‡½æ•°æµ‹è¯• ===\n")
    
    # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
    test_doc = """
    æ›¹æ“åˆ†å…µæ”¶å‰²å†›ç²®åï¼Œå•å¸ƒé™ˆå®«çœ‹åˆ°æ›¹æ“å…µåŠ›åˆ†æ•£ï¼Œè®¤ä¸ºæ˜¯å¥½æœºä¼šï¼Œåˆç‡é¢†äº†ä¸€ä¸‡äººé©¬è¦æ¥å’Œæ›¹æ“å†³ä¸€æ­»æˆ˜ã€‚
    ç»“æœï¼Œè¢«æ›¹æ“è®¾ä¸‹åŸ‹ä¼æ€å¾—å¤§è´¥ã€‚è¿™ä¸€ä»—å•å¸ƒä¼¤ç­‹åŠ¨éª¨ï¼Œä»æ­¤å†ä¹Ÿæ— æ³•åœ¨å…–å·ç«‹è¶³ã€‚
    
    å…–å·çš„å›ä¹±ï¼Œè¡¨é¢åŸå› æ˜¯å› ä¸ºæ›¹æ“è¿‡äºç²—æš´ï¼Œæ€å®³äº†é™ˆç•™çš„åå£«è¾¹è®©ï¼Œå¼•èµ·äº†å…–å·äººå£«çš„ææ…Œã€‚
    å®é™…ä¸Šæœ‰æ›´æ·±åˆ»çš„åŸå› ã€‚çœ‹ä¸€çœ‹å„åœ°çš„è¯¸ä¾¯ï¼Œæ›¹æ“åœ¨å…–å·ï¼Œåˆ˜å¤‡åœ¨å¾å·ï¼Œå­™ç­–åœ¨æ±Ÿå—ã€‚
    
    æ›¹æ“å†ç»äº†å…–å·å›ä¹±ï¼Œåˆ˜å¤‡é­é‡äº†å•å¸ƒåæ°´ï¼Œç»“æœéƒ½æ˜¯å› ä¸ºæ²»ç†å†…éƒ¨çš„æ—¶é—´å¤ªçŸ­ï¼Œå·¥å¤«è¿˜æ²¡åˆ°ã€‚
    åæ¥æ›¹æ“ä¹Ÿæ€äº†å­”èï¼Œå­”èçš„åæ°”å¯æ¯”è¾¹è®©å¤§å¾—å¤šï¼Œä½†ä»€ä¹ˆäº‹ä¹Ÿæ²¡æœ‰ã€‚
    """
    
    doc_id = compute_mdhash_id(test_doc.strip(), prefix="doc-")
    print(f"ğŸ“„ æµ‹è¯•æ–‡æ¡£ID: {doc_id}")
    print(f"ğŸ“„ æ–‡æ¡£é•¿åº¦: {len(test_doc)} å­—ç¬¦\n")
    
    # åˆ›å»ºtiktokenç¼–ç å™¨
    encoder = tiktoken.encoding_for_model("gpt-4o")
    
    # ç¼–ç æ–‡æ¡£ä¸ºtokens
    tokens = encoder.encode(test_doc)
    print(f"ğŸ“Š ç¼–ç åtokenæ•°é‡: {len(tokens)}")
    print()
    
    # ========== æµ‹è¯•1: chunking_by_token_size ==========
    print("ğŸ”§ æµ‹è¯•1: chunking_by_token_size() å‡½æ•°")
    print("-" * 50)
    
    token_chunks = chunking_by_token_size(
        tokens_list=[tokens],           # åŒ…è£…ä¸ºåˆ—è¡¨
        doc_keys=[doc_id],             # æ–‡æ¡£IDåˆ—è¡¨
        tiktoken_model=encoder,        # tiktokenç¼–ç å™¨
        overlap_token_size=50,         # é‡å 50ä¸ªtoken
        max_token_size=200,           # æ¯å—æœ€å¤§200ä¸ªtoken
    )
    
    print(f"âœ… ç”Ÿæˆåˆ†å—æ•°é‡: {len(token_chunks)}")
    for i, chunk in enumerate(token_chunks):
        print(f"  åˆ†å— {i+1}:")
        print(f"    Tokenæ•°é‡: {chunk['tokens']}")
        print(f"    åˆ†å—ç´¢å¼•: {chunk['chunk_order_index']}")
        print(f"    æ–‡æ¡£ID: {chunk['full_doc_id']}")
        print(f"    å†…å®¹é¢„è§ˆ: {chunk['content'][:100]}...")
        print()
    
    # ========== æµ‹è¯•2: chunking_by_seperators ==========
    print("ğŸ”§ æµ‹è¯•2: chunking_by_seperators() å‡½æ•°")
    print("-" * 50)
    
    separator_chunks = chunking_by_seperators(
        tokens_list=[tokens],
        doc_keys=[doc_id],
        tiktoken_model=encoder,
        overlap_token_size=30,
        max_token_size=150,
    )
    
    print(f"âœ… ç”Ÿæˆåˆ†å—æ•°é‡: {len(separator_chunks)}")
    for i, chunk in enumerate(separator_chunks):
        print(f"  åˆ†å— {i+1}:")
        print(f"    Tokenæ•°é‡: {chunk['tokens']}")
        print(f"    åˆ†å—ç´¢å¼•: {chunk['chunk_order_index']}")
        print(f"    æ–‡æ¡£ID: {chunk['full_doc_id']}")
        print(f"    å†…å®¹é¢„è§ˆ: {chunk['content'][:100]}...")
        print()
    
    # ========== æµ‹è¯•3: get_chunks ç»Ÿä¸€æ¥å£ ==========
    print("ğŸ”§ æµ‹è¯•3: get_chunks() ç»Ÿä¸€æ¥å£å‡½æ•°")
    print("-" * 50)
    
    # å‡†å¤‡è¾“å…¥æ ¼å¼ï¼ˆæ¨¡æ‹ŸGraphRAGå†…éƒ¨æ ¼å¼ï¼‰
    new_docs = {
        doc_id: {"content": test_doc.strip()}
    }
    
    # ä½¿ç”¨get_chunkså‡½æ•°
    result_chunks = get_chunks(
        new_docs=new_docs,
        chunk_func=chunking_by_token_size,  # æŒ‡å®šåˆ†å—å‡½æ•°
        overlap_token_size=40,
        max_token_size=180,
    )
    
    print(f"âœ… ç”Ÿæˆåˆ†å—æ•°é‡: {len(result_chunks)}")
    print("ğŸ“‹ åˆ†å—ç»“æœå­—å…¸æ ¼å¼:")
    
    for chunk_id, chunk_data in result_chunks.items():
        print(f"  åˆ†å—ID: {chunk_id}")
        print(f"    Tokenæ•°é‡: {chunk_data['tokens']}")
        print(f"    åˆ†å—ç´¢å¼•: {chunk_data['chunk_order_index']}")
        print(f"    æ–‡æ¡£ID: {chunk_data['full_doc_id']}")
        print(f"    å†…å®¹é¢„è§ˆ: {chunk_data['content'][:100]}...")
        print()
    
    # ========== ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ ==========
    print("ğŸ’¾ ä¿å­˜åˆ†å—ç»“æœåˆ°æ–‡ä»¶")
    print("-" * 50)
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆæ¨¡æ‹Ÿkv_store_text_chunks.jsonï¼‰
    with open("test_chunks_result.json", "w", encoding="utf-8") as f:
        json.dump(result_chunks, f, indent=2, ensure_ascii=False)
    
    print("âœ… åˆ†å—ç»“æœå·²ä¿å­˜åˆ°: test_chunks_result.json")
    
    # ========== ä¸å®é™…kv_store_text_chunks.jsonå¯¹æ¯” ==========
    print("\nğŸ“Š ä¸å®é™…ç”Ÿæˆæ–‡ä»¶çš„æ•°æ®ç»“æ„å¯¹æ¯”")
    print("-" * 50)
    
    try:
        with open("mytest/kv_store_text_chunks.json", "r", encoding="utf-8") as f:
            actual_chunks = json.load(f)
        
        print(f"å®é™…æ–‡ä»¶åˆ†å—æ•°é‡: {len(actual_chunks)}")
        print("å®é™…æ–‡ä»¶æ•°æ®ç»“æ„ç¤ºä¾‹:")
        
        for i, (chunk_id, chunk_data) in enumerate(actual_chunks.items()):
            if i >= 1:  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ª
                break
            print(f"  åˆ†å—ID: {chunk_id}")
            print(f"    tokens: {chunk_data['tokens']}")
            print(f"    chunk_order_index: {chunk_data['chunk_order_index']}")
            print(f"    full_doc_id: {chunk_data['full_doc_id']}")
            print(f"    content: {chunk_data['content'][:100]}...")
            
    except FileNotFoundError:
        print("âš ï¸  æœªæ‰¾åˆ°å®é™…çš„ kv_store_text_chunks.json æ–‡ä»¶")
    
    print("\nğŸ¯ æ€»ç»“")
    print("-" * 50)
    print("âœ… æ–‡æœ¬åˆ†å—å‡½æ•°æµ‹è¯•å®Œæˆ")
    print("âœ… æ•°æ®ç»“æ„ä¸GraphRAGä¿æŒä¸€è‡´")
    print("âœ… æ”¯æŒTokenåˆ†å—å’Œåˆ†éš”ç¬¦åˆ†å—")
    print("âœ… åˆ†å—IDä½¿ç”¨MD5å“ˆå¸Œç¡®ä¿å”¯ä¸€æ€§")
    print("âœ… ä¿æŒåŸæ–‡æ¡£è¿½è¸ªå’Œåˆ†å—é¡ºåº")


if __name__ == "__main__":
    test_chunking_functions() 