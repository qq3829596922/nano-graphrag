# æ–‡æœ¬åˆ†å—å‡½æ•°è¯¦ç»†è§£æ

## ğŸ“ æ–‡æœ¬åˆ†å—åŠŸèƒ½åœ¨é¡¹ç›®ä¸­çš„ä½ç½®

### æ ¸å¿ƒæ–‡ä»¶ä½ç½®

| æ–‡ä»¶è·¯å¾„ | ä½œç”¨ | å…³é”®å‡½æ•°/ç±» |
|---------|------|------------|
| `nano_graphrag/_op.py` | ä¸»è¦åˆ†å—é€»è¾‘å®ç° | `chunking_by_token_size()`, `chunking_by_seperators()`, `get_chunks()` |
| `nano_graphrag/_splitter.py` | åˆ†éš”ç¬¦åˆ†å‰²å™¨ | `SeparatorSplitter` ç±» |
| `nano_graphrag/prompt.py` | åˆ†éš”ç¬¦é…ç½® | `PROMPTS["default_text_separator"]` |
| `nano_graphrag/graphrag.py` | åˆ†å—å‡½æ•°è°ƒç”¨ | `GraphRAG.__post_init__()`, `ainsert()` |

## ğŸ”§ æ ¸å¿ƒåˆ†å—å‡½æ•°å®ç°

### 1. `chunking_by_token_size()` - åŸºäºTokenæ•°é‡åˆ†å—

**ä½ç½®**: `nano_graphrag/_op.py:32-60`

```python
def chunking_by_token_size(
    tokens_list: list[list[int]],    # å·²ç¼–ç çš„tokenåˆ—è¡¨
    doc_keys,                        # æ–‡æ¡£é”®ååˆ—è¡¨
    tiktoken_model,                  # tiktokenç¼–ç å™¨
    overlap_token_size=128,          # é‡å tokenæ•°é‡ï¼ˆé»˜è®¤128ï¼‰
    max_token_size=1024,            # æœ€å¤§åˆ†å—å¤§å°ï¼ˆé»˜è®¤1024ï¼‰
):
    """
    åŸºäºTokenæ•°é‡çš„æ–‡æ¡£åˆ†å—æ–¹æ³•
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. éå†æ¯ä¸ªæ–‡æ¡£çš„tokenåˆ—è¡¨
    2. ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹å¼è¿›è¡Œåˆ†å—
    3. è®¾ç½®é‡å åŒºåŸŸé¿å…ä¿¡æ¯ä¸¢å¤±
    4. æ‰¹é‡è§£ç tokenä¸ºæ–‡æœ¬
    5. æ„å»ºåˆ†å—ç»“æœæ•°æ®ç»“æ„
    """
    results = []
    for index, tokens in enumerate(tokens_list):
        chunk_token = []
        lengths = []
        
        # æ»‘åŠ¨çª—å£åˆ†å—ï¼šæ¯æ¬¡å‰è¿› (max_token_size - overlap_token_size) ä¸ªtoken
        for start in range(0, len(tokens), max_token_size - overlap_token_size):
            chunk_token.append(tokens[start : start + max_token_size])
            lengths.append(min(max_token_size, len(tokens) - start))

        # æ‰¹é‡è§£ç tokenä¸ºæ–‡æœ¬ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        chunk_token = tiktoken_model.decode_batch(chunk_token)
        
        # æ„å»ºç»“æœæ•°æ®ç»“æ„
        for i, chunk in enumerate(chunk_token):
            results.append({
                "tokens": lengths[i],              # Tokenæ•°é‡
                "content": chunk.strip(),          # åˆ†å—å†…å®¹
                "chunk_order_index": i,            # åˆ†å—ç´¢å¼•
                "full_doc_id": doc_keys[index],    # åŸæ–‡æ¡£ID
            })

    return results
```

**å…³é”®ç‰¹ç‚¹ï¼š**
- âœ… ç²¾ç¡®æ§åˆ¶åˆ†å—å¤§å°
- âœ… æ”¯æŒé‡å åŒºåŸŸï¼ˆé˜²æ­¢å…³é”®ä¿¡æ¯è¢«æˆªæ–­ï¼‰
- âœ… æ‰¹é‡å¤„ç†æé«˜æ€§èƒ½
- âœ… ä¿æŒæ–‡æ¡£æ¥æºè¿½è¸ª

### 2. `chunking_by_seperators()` - åŸºäºåˆ†éš”ç¬¦åˆ†å—

**ä½ç½®**: `nano_graphrag/_op.py:62-90`

```python
def chunking_by_seperators(
    tokens_list: list[list[int]],
    doc_keys,
    tiktoken_model,
    overlap_token_size=128,
    max_token_size=1024,
):
    """
    åŸºäºåˆ†éš”ç¬¦çš„æ™ºèƒ½åˆ†å—æ–¹æ³•
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. åˆ›å»ºSeparatorSplitterå®ä¾‹
    2. ä½¿ç”¨é¢„å®šä¹‰çš„åˆ†éš”ç¬¦è¿›è¡Œåˆ†å‰²
    3. æ™ºèƒ½åˆå¹¶å°ç‰‡æ®µ
    4. ç¡®ä¿åˆ†å—å¤§å°åœ¨åˆç†èŒƒå›´
    """
    
    # åˆ›å»ºåˆ†éš”ç¬¦åˆ†å‰²å™¨
    splitter = SeparatorSplitter(
        separators=[
            tiktoken_model.encode(s) for s in PROMPTS["default_text_separator"]
        ],
        chunk_size=max_token_size,
        chunk_overlap=overlap_token_size,
    )
    
    results = []
    for index, tokens in enumerate(tokens_list):
        # ä½¿ç”¨åˆ†å‰²å™¨è¿›è¡Œæ™ºèƒ½åˆ†å—
        chunk_token = splitter.split_tokens(tokens)
        lengths = [len(c) for c in chunk_token]

        # è§£ç å¹¶æ„å»ºç»“æœ
        chunk_token = tiktoken_model.decode_batch(chunk_token)
        for i, chunk in enumerate(chunk_token):
            results.append({
                "tokens": lengths[i],
                "content": chunk.strip(),
                "chunk_order_index": i,
                "full_doc_id": doc_keys[index],
            })

    return results
```

**é»˜è®¤åˆ†éš”ç¬¦é…ç½®**ï¼ˆ`nano_graphrag/prompt.py:498-521`ï¼‰ï¼š

```python
PROMPTS["default_text_separator"] = [
    # æ®µè½åˆ†éš”ç¬¦
    "\n\n",        # åŒæ¢è¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰
    "\r\n\r\n",    # Windowsæ®µè½åˆ†éš”
    
    # è¡Œåˆ†éš”ç¬¦
    "\n",          # å•æ¢è¡Œ
    "\r\n",        # Windowsæ¢è¡Œ
    
    # å¥å­ç»“æŸæ ‡ç‚¹
    "ã€‚",          # ä¸­æ–‡å¥å·
    "ï¼",          # å…¨è§’å¥å·
    ".",           # è‹±æ–‡å¥å·
    "ï¼",          # ä¸­æ–‡æ„Ÿå¹å·
    "!",           # è‹±æ–‡æ„Ÿå¹å·
    "ï¼Ÿ",          # ä¸­æ–‡é—®å·
    "?",           # è‹±æ–‡é—®å·
    
    # ç©ºç™½å­—ç¬¦
    " ",           # ç©ºæ ¼
    "\t",          # åˆ¶è¡¨ç¬¦
    "\u3000",      # å…¨è§’ç©ºæ ¼
    
    # ç‰¹æ®Šå­—ç¬¦
    "\u200b",      # é›¶å®½ç©ºæ ¼ï¼ˆç”¨äºäºšæ´²è¯­è¨€ï¼‰
]
```

### 3. `get_chunks()` - åˆ†å—ç»Ÿä¸€å…¥å£å‡½æ•°

**ä½ç½®**: `nano_graphrag/_op.py:92-119`

```python
def get_chunks(new_docs, chunk_func=chunking_by_token_size, **chunk_func_params):
    """
    æ–‡æ¡£åˆ†å—çš„ç»Ÿä¸€å…¥å£å‡½æ•°
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. è§£æè¾“å…¥æ–‡æ¡£
    2. æ‰¹é‡ç¼–ç ä¸ºtoken
    3. è°ƒç”¨æŒ‡å®šçš„åˆ†å—å‡½æ•°
    4. ç”Ÿæˆå”¯ä¸€çš„åˆ†å—ID
    5. è¿”å›åˆ†å—å­—å…¸
    """
    inserting_chunks = {}

    # è§£ææ–‡æ¡£æ•°æ®
    new_docs_list = list(new_docs.items())
    docs = [new_doc[1]["content"] for new_doc in new_docs_list]
    doc_keys = [new_doc[0] for new_doc in new_docs_list]

    # æ‰¹é‡ç¼–ç ï¼ˆå¤šçº¿ç¨‹ä¼˜åŒ–ï¼‰
    ENCODER = tiktoken.encoding_for_model("gpt-4o")
    tokens = ENCODER.encode_batch(docs, num_threads=16)
    
    # è°ƒç”¨åˆ†å—å‡½æ•°
    chunks = chunk_func(
        tokens, 
        doc_keys=doc_keys, 
        tiktoken_model=ENCODER, 
        **chunk_func_params
    )

    # ä¸ºæ¯ä¸ªåˆ†å—ç”Ÿæˆå”¯ä¸€ID
    for chunk in chunks:
        inserting_chunks.update({
            compute_mdhash_id(chunk["content"], prefix="chunk-"): chunk
        })

    return inserting_chunks
```

### 4. `SeparatorSplitter` ç±» - æ™ºèƒ½åˆ†éš”ç¬¦å¤„ç†

**ä½ç½®**: `nano_graphrag/_splitter.py:3-95`

```python
class SeparatorSplitter:
    """
    åŸºäºåˆ†éš”ç¬¦çš„æ–‡æœ¬åˆ†å‰²å™¨
    æ”¯æŒå¤šä¸ªåˆ†éš”ç¬¦ä¼˜å…ˆçº§ã€åˆ†å—å¤§å°æ§åˆ¶å’Œé‡å è®¾ç½®
    """
    
    def __init__(self, separators, chunk_size=4000, chunk_overlap=200, ...):
        # åˆå§‹åŒ–åˆ†å‰²å™¨å‚æ•°
        
    def split_tokens(self, tokens: List[int]) -> List[List[int]]:
        """ä¸»è¦åˆ†å‰²æ–¹æ³•"""
        splits = self._split_tokens_with_separators(tokens)
        return self._merge_splits(splits)
    
    def _split_tokens_with_separators(self, tokens):
        """ä½¿ç”¨åˆ†éš”ç¬¦åˆ†å‰²token"""
        # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾åˆ†éš”ç¬¦å¹¶åˆ†å‰²
        
    def _merge_splits(self, splits):
        """åˆå¹¶å°ç‰‡æ®µä¸ºåˆé€‚å¤§å°çš„åˆ†å—"""
        # æ™ºèƒ½åˆå¹¶é€»è¾‘
        
    def _enforce_overlap(self, chunks):
        """æ·»åŠ åˆ†å—é‡å """
        # é‡å å¤„ç†é€»è¾‘
```

## ğŸ“Š kv_store_text_chunks.json æ–‡ä»¶ç»“æ„åˆ†æ

### æ–‡ä»¶ä½ç½®
`./mytest/kv_store_text_chunks.json`

### æ•°æ®ç»“æ„è¯¦è§£

```json
{
  "chunk-{å“ˆå¸ŒID}": {
    "tokens": 1200,                    // åˆ†å—çš„Tokenæ•°é‡
    "content": "æ–‡æ¡£å†…å®¹...",           // åˆ†å—çš„å®é™…æ–‡æœ¬å†…å®¹
    "chunk_order_index": 0,            // åœ¨åŸæ–‡æ¡£ä¸­çš„åˆ†å—é¡ºåºï¼ˆä»0å¼€å§‹ï¼‰
    "full_doc_id": "doc-{æ–‡æ¡£å“ˆå¸ŒID}"  // åŸå§‹æ–‡æ¡£çš„å”¯ä¸€æ ‡è¯†ç¬¦
  }
}
```

### å®é™…æ•°æ®ç¤ºä¾‹

```json
{
  "chunk-4fff7251c7c747260398bf3bac7f0a6f": {
    "tokens": 1200,
    "content": "æ›¹æ“åˆ†å…µæ”¶å‰²å†›ç²®åï¼Œå•å¸ƒé™ˆå®«çœ‹åˆ°æ›¹æ“å…µåŠ›åˆ†æ•£...",
    "chunk_order_index": 0,
    "full_doc_id": "doc-5847bc92a3a6d4ae3daac921d8fcfe70"
  },
  "chunk-401872faa589e7456e18805b866a25e1": {
    "tokens": 1200,
    "content": "æ°”ï¼Œå…´å¹³äºŒå¹´ï¼ˆå…¬å…ƒ195å¹´ï¼‰äºŒæœˆå¹²è„†è¶å¬å¼€å†›äº‹ä¼šè®®...",
    "chunk_order_index": 1,
    "full_doc_id": "doc-5847bc92a3a6d4ae3daac921d8fcfe70"
  },
  "chunk-36d6084250ac83de36e517351a390afd": {
    "tokens": 322,
    "content": "ï¼Ÿè¿™ä¸ªé€ƒäº¡ä¸­çš„å°æœå»·åœ¨å¦‚æ­¤çª˜è¿«çš„æƒ…å†µä¸‹...",
    "chunk_order_index": 2,
    "full_doc_id": "doc-5847bc92a3a6d4ae3daac921d8fcfe70"
  }
}
```

### å­—æ®µè§£æ

| å­—æ®µå | ç±»å‹ | ä½œç”¨ | ç¤ºä¾‹ |
|--------|------|------|------|
| `å“ˆå¸ŒID` | string | åˆ†å—çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œæ ¼å¼: `chunk-{MD5å“ˆå¸Œ}` | `chunk-4fff7251c7c747260398bf3bac7f0a6f` |
| `tokens` | integer | è¯¥åˆ†å—åŒ…å«çš„Tokenæ•°é‡ | `1200` |
| `content` | string | åˆ†å—çš„æ–‡æœ¬å†…å®¹ | `"æ›¹æ“åˆ†å…µæ”¶å‰²å†›ç²®å..."` |
| `chunk_order_index` | integer | åœ¨åŸæ–‡æ¡£ä¸­çš„é¡ºåºç´¢å¼• | `0`, `1`, `2` |
| `full_doc_id` | string | åŸå§‹æ–‡æ¡£IDï¼Œæ ¼å¼: `doc-{MD5å“ˆå¸Œ}` | `doc-5847bc92a3a6d4ae3daac921d8fcfe70` |

## ğŸ”„ åˆ†å—è¿‡ç¨‹åœ¨ insert æ–¹æ³•ä¸­çš„è°ƒç”¨

### åœ¨ GraphRAG.ainsert() ä¸­çš„æ‰§è¡Œ

```python
# æ­¥éª¤2: æ–‡æœ¬åˆ†å—å¤„ç†
inserting_chunks = get_chunks(
    new_docs=new_docs,                              # è¾“å…¥æ–‡æ¡£
    chunk_func=self.chunk_func,                     # åˆ†å—å‡½æ•°ï¼ˆé»˜è®¤: chunking_by_token_sizeï¼‰
    overlap_token_size=self.chunk_overlap_token_size, # é‡å tokenæ•°ï¼ˆé»˜è®¤: 100ï¼‰
    max_token_size=self.chunk_token_size,           # æœ€å¤§åˆ†å—å¤§å°ï¼ˆé»˜è®¤: 1200ï¼‰
)
```

### é…ç½®å‚æ•°

åœ¨ `GraphRAG` ç±»ä¸­çš„é»˜è®¤é…ç½®ï¼š

```python
@dataclass
class GraphRAG:
    # åˆ†å—ç›¸å…³é…ç½®
    chunk_func: Callable = chunking_by_token_size  # åˆ†å—å‡½æ•°
    chunk_token_size: int = 1200                   # åˆ†å—å¤§å°
    chunk_overlap_token_size: int = 100            # é‡å å¤§å°
    tiktoken_model_name: str = "gpt-4o"            # Tokenè®¡ç®—æ¨¡å‹
```

## ğŸ¯ åˆ†å—ç®—æ³•å¯¹æ¯”

### Token åˆ†å— vs åˆ†éš”ç¬¦åˆ†å—

| ç‰¹æ€§ | Tokenåˆ†å— | åˆ†éš”ç¬¦åˆ†å— |
|------|-----------|------------|
| **ç²¾ç¡®åº¦** | ç²¾ç¡®æ§åˆ¶å¤§å° | æ ¹æ®è¯­ä¹‰è¾¹ç•Œ |
| **è¯­ä¹‰å®Œæ•´æ€§** | å¯èƒ½æˆªæ–­å¥å­ | ä¿æŒè¯­ä¹‰å®Œæ•´ |
| **å¤„ç†é€Ÿåº¦** | å¿«é€Ÿ | ç¨æ…¢ï¼ˆéœ€è¦æŸ¥æ‰¾åˆ†éš”ç¬¦ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | ä¸¥æ ¼å¤§å°æ§åˆ¶ | éœ€è¦è¯­ä¹‰å®Œæ•´æ€§ |
| **é‡å å¤„ç†** | ç®€å•æ»‘åŠ¨çª—å£ | æ™ºèƒ½è¾¹ç•Œå¯¹é½ |

### å®é™…åˆ†å—ç»“æœå¯¹æ¯”

ä» `kv_store_text_chunks.json` å¯ä»¥çœ‹åˆ°ï¼Œä¸‰å›½æ¼”ä¹‰æ–‡æ¡£è¢«åˆ†æˆäº†3ä¸ªåˆ†å—ï¼š

1. **åˆ†å—1**: 1200 tokens - ä»"æ›¹æ“åˆ†å…µæ”¶å‰²å†›ç²®"å¼€å§‹
2. **åˆ†å—2**: 1200 tokens - ä»"æ°”ï¼Œå…´å¹³äºŒå¹´"å¼€å§‹ï¼ˆæœ‰é‡å ï¼‰
3. **åˆ†å—3**: 322 tokens - æ–‡æ¡£ç»“å°¾éƒ¨åˆ†

## ğŸ“š æ–‡æ¡£åˆ†å—çš„é‡è¦ä½œç”¨

### 1. ä¸ºåç»­å¤„ç†åšå‡†å¤‡
- **å®ä½“æå–**: LLM åˆ†ææ¯ä¸ªåˆ†å—æå–å®ä½“å’Œå…³ç³»
- **å‘é‡åŒ–**: å°†åˆ†å—è½¬æ¢ä¸ºå‘é‡ç”¨äºç›¸ä¼¼åº¦æœç´¢
- **ç´¢å¼•æ„å»º**: å»ºç«‹åˆ†å—åˆ°åŸæ–‡æ¡£çš„æ˜ å°„å…³ç³»

### 2. æ€§èƒ½ä¼˜åŒ–
- **å¹¶è¡Œå¤„ç†**: å¤šä¸ªåˆ†å—å¯ä»¥å¹¶è¡Œè¿›è¡Œå®ä½“æå–
- **å†…å­˜æ§åˆ¶**: é¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤§çš„æ–‡æ¡£
- **API è°ƒç”¨ä¼˜åŒ–**: åˆ†å—å¤§å°é€‚åˆ LLM çš„ä¸Šä¸‹æ–‡çª—å£

### 3. æŸ¥è¯¢æ”¯æŒ
- **ç²¾ç¡®æ£€ç´¢**: æ ¹æ®åˆ†å—IDå¿«é€Ÿå®šä½å…·ä½“å†…å®¹
- **ç›¸å…³æ€§è®¡ç®—**: åŸºäºåˆ†å—çº§åˆ«çš„ç›¸ä¼¼åº¦åŒ¹é…
- **ä¸Šä¸‹æ–‡æ„å»º**: ä¸ºæŸ¥è¯¢æ„å»ºç›¸å…³çš„æ–‡æœ¬ä¸Šä¸‹æ–‡

## ğŸ› ï¸ è‡ªå®šä¹‰åˆ†å—æ–¹æ³•

### å¦‚ä½•æ›¿æ¢åˆ†å—å‡½æ•°

```python
# è‡ªå®šä¹‰åˆ†å—å‡½æ•°
def my_custom_chunking(tokens_list, doc_keys, tiktoken_model, **params):
    # å®ç°è‡ªå·±çš„åˆ†å—é€»è¾‘
    pass

# ä½¿ç”¨è‡ªå®šä¹‰åˆ†å—å‡½æ•°
graph_func = GraphRAG(
    working_dir="./mytest",
    chunk_func=my_custom_chunking,  # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
    chunk_token_size=800,          # è‡ªå®šä¹‰åˆ†å—å¤§å°
    chunk_overlap_token_size=50,   # è‡ªå®šä¹‰é‡å å¤§å°
)
```

### ç¤ºä¾‹ï¼šåŸºäºæ®µè½çš„åˆ†å—

```python
def chunking_by_paragraphs(tokens_list, doc_keys, tiktoken_model, max_token_size=1024):
    """åŸºäºæ®µè½çš„åˆ†å—æ–¹æ³•"""
    results = []
    
    for index, tokens in enumerate(tokens_list):
        # è§£ç ä¸ºæ–‡æœ¬
        text = tiktoken_model.decode(tokens)
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        chunk_index = 0
        
        for paragraph in paragraphs:
            para_tokens = tiktoken_model.encode(paragraph)
            
            if len(tiktoken_model.encode(current_chunk + paragraph)) <= max_token_size:
                current_chunk += paragraph + '\n\n'
            else:
                if current_chunk:
                    results.append({
                        "tokens": len(tiktoken_model.encode(current_chunk)),
                        "content": current_chunk.strip(),
                        "chunk_order_index": chunk_index,
                        "full_doc_id": doc_keys[index],
                    })
                    chunk_index += 1
                current_chunk = paragraph + '\n\n'
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk:
            results.append({
                "tokens": len(tiktoken_model.encode(current_chunk)),
                "content": current_chunk.strip(),
                "chunk_order_index": chunk_index,
                "full_doc_id": doc_keys[index],
            })
    
    return results
```

## ğŸ’¡ æ€»ç»“

æ–‡æœ¬åˆ†å—æ˜¯ GraphRAG ç³»ç»Ÿçš„åŸºç¡€ç¯èŠ‚ï¼Œå®ƒï¼š

1. **ä½äºå…³é”®è·¯å¾„**: æ˜¯ insert æ–¹æ³•çš„ç¬¬äºŒæ­¥ï¼Œä¸ºåç»­æ‰€æœ‰å¤„ç†å¥ å®šåŸºç¡€
2. **æ”¯æŒå¤šç§ç­–ç•¥**: Tokenåˆ†å—å’Œåˆ†éš”ç¬¦åˆ†å—ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©
3. **æ€§èƒ½ä¼˜åŒ–**: ä½¿ç”¨æ‰¹é‡å¤„ç†å’Œå¤šçº¿ç¨‹ç¼–ç æé«˜æ•ˆç‡
4. **æ•°æ®å®Œæ•´æ€§**: é€šè¿‡é‡å æœºåˆ¶é¿å…é‡è¦ä¿¡æ¯ä¸¢å¤±
5. **å¯æ‰©å±•æ€§**: æ”¯æŒè‡ªå®šä¹‰åˆ†å—å‡½æ•°ï¼Œæ»¡è¶³ç‰¹æ®Šéœ€æ±‚

ç”Ÿæˆçš„ `kv_store_text_chunks.json` æ–‡ä»¶æ˜¯æ•´ä¸ªçŸ¥è¯†å›¾è°±æ„å»ºè¿‡ç¨‹çš„é‡è¦ä¸­é—´äº§ç‰©ï¼Œä¸ºå®ä½“æå–ã€å…³ç³»è¯†åˆ«å’ŒæŸ¥è¯¢æ£€ç´¢æä¾›äº†ç»“æ„åŒ–çš„æ–‡æœ¬åŸºç¡€ã€‚ 