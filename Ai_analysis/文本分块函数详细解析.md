# 文本分块函数详细解析

## 📁 文本分块功能在项目中的位置

### 核心文件位置

| 文件路径 | 作用 | 关键函数/类 |
|---------|------|------------|
| `nano_graphrag/_op.py` | 主要分块逻辑实现 | `chunking_by_token_size()`, `chunking_by_seperators()`, `get_chunks()` |
| `nano_graphrag/_splitter.py` | 分隔符分割器 | `SeparatorSplitter` 类 |
| `nano_graphrag/prompt.py` | 分隔符配置 | `PROMPTS["default_text_separator"]` |
| `nano_graphrag/graphrag.py` | 分块函数调用 | `GraphRAG.__post_init__()`, `ainsert()` |

## 🔧 核心分块函数实现

### 1. `chunking_by_token_size()` - 基于Token数量分块

**位置**: `nano_graphrag/_op.py:32-60`

```python
def chunking_by_token_size(
    tokens_list: list[list[int]],    # 已编码的token列表
    doc_keys,                        # 文档键名列表
    tiktoken_model,                  # tiktoken编码器
    overlap_token_size=128,          # 重叠token数量（默认128）
    max_token_size=1024,            # 最大分块大小（默认1024）
):
    """
    基于Token数量的文档分块方法
    
    执行流程：
    1. 遍历每个文档的token列表
    2. 使用滑动窗口方式进行分块
    3. 设置重叠区域避免信息丢失
    4. 批量解码token为文本
    5. 构建分块结果数据结构
    """
    results = []
    for index, tokens in enumerate(tokens_list):
        chunk_token = []
        lengths = []
        
        # 滑动窗口分块：每次前进 (max_token_size - overlap_token_size) 个token
        for start in range(0, len(tokens), max_token_size - overlap_token_size):
            chunk_token.append(tokens[start : start + max_token_size])
            lengths.append(min(max_token_size, len(tokens) - start))

        # 批量解码token为文本（性能优化）
        chunk_token = tiktoken_model.decode_batch(chunk_token)
        
        # 构建结果数据结构
        for i, chunk in enumerate(chunk_token):
            results.append({
                "tokens": lengths[i],              # Token数量
                "content": chunk.strip(),          # 分块内容
                "chunk_order_index": i,            # 分块索引
                "full_doc_id": doc_keys[index],    # 原文档ID
            })

    return results
```

**关键特点：**
- ✅ 精确控制分块大小
- ✅ 支持重叠区域（防止关键信息被截断）
- ✅ 批量处理提高性能
- ✅ 保持文档来源追踪

### 2. `chunking_by_seperators()` - 基于分隔符分块

**位置**: `nano_graphrag/_op.py:62-90`

```python
def chunking_by_seperators(
    tokens_list: list[list[int]],
    doc_keys,
    tiktoken_model,
    overlap_token_size=128,
    max_token_size=1024,
):
    """
    基于分隔符的智能分块方法
    
    执行流程：
    1. 创建SeparatorSplitter实例
    2. 使用预定义的分隔符进行分割
    3. 智能合并小片段
    4. 确保分块大小在合理范围
    """
    
    # 创建分隔符分割器
    splitter = SeparatorSplitter(
        separators=[
            tiktoken_model.encode(s) for s in PROMPTS["default_text_separator"]
        ],
        chunk_size=max_token_size,
        chunk_overlap=overlap_token_size,
    )
    
    results = []
    for index, tokens in enumerate(tokens_list):
        # 使用分割器进行智能分块
        chunk_token = splitter.split_tokens(tokens)
        lengths = [len(c) for c in chunk_token]

        # 解码并构建结果
        chunk_token = tiktoken_model.decode_batch(chunk_token)
        for i, chunk in enumerate(chunk_token):
            results.append({
                "tokens": lengths[i],
                "content": chunk.strip(),
                "chunk_order_index": i,
                "full_doc_id": doc_keys[index],
            })

    return results
```

**默认分隔符配置**（`nano_graphrag/prompt.py:498-521`）：

```python
PROMPTS["default_text_separator"] = [
    # 段落分隔符
    "\n\n",        # 双换行（段落分隔）
    "\r\n\r\n",    # Windows段落分隔
    
    # 行分隔符
    "\n",          # 单换行
    "\r\n",        # Windows换行
    
    # 句子结束标点
    "。",          # 中文句号
    "．",          # 全角句号
    ".",           # 英文句号
    "！",          # 中文感叹号
    "!",           # 英文感叹号
    "？",          # 中文问号
    "?",           # 英文问号
    
    # 空白字符
    " ",           # 空格
    "\t",          # 制表符
    "\u3000",      # 全角空格
    
    # 特殊字符
    "\u200b",      # 零宽空格（用于亚洲语言）
]
```

### 3. `get_chunks()` - 分块统一入口函数

**位置**: `nano_graphrag/_op.py:92-119`

```python
def get_chunks(new_docs, chunk_func=chunking_by_token_size, **chunk_func_params):
    """
    文档分块的统一入口函数
    
    执行流程：
    1. 解析输入文档
    2. 批量编码为token
    3. 调用指定的分块函数
    4. 生成唯一的分块ID
    5. 返回分块字典
    """
    inserting_chunks = {}

    # 解析文档数据
    new_docs_list = list(new_docs.items())
    docs = [new_doc[1]["content"] for new_doc in new_docs_list]
    doc_keys = [new_doc[0] for new_doc in new_docs_list]

    # 批量编码（多线程优化）
    ENCODER = tiktoken.encoding_for_model("gpt-4o")
    tokens = ENCODER.encode_batch(docs, num_threads=16)
    
    # 调用分块函数
    chunks = chunk_func(
        tokens, 
        doc_keys=doc_keys, 
        tiktoken_model=ENCODER, 
        **chunk_func_params
    )

    # 为每个分块生成唯一ID
    for chunk in chunks:
        inserting_chunks.update({
            compute_mdhash_id(chunk["content"], prefix="chunk-"): chunk
        })

    return inserting_chunks
```

### 4. `SeparatorSplitter` 类 - 智能分隔符处理

**位置**: `nano_graphrag/_splitter.py:3-95`

```python
class SeparatorSplitter:
    """
    基于分隔符的文本分割器
    支持多个分隔符优先级、分块大小控制和重叠设置
    """
    
    def __init__(self, separators, chunk_size=4000, chunk_overlap=200, ...):
        # 初始化分割器参数
        
    def split_tokens(self, tokens: List[int]) -> List[List[int]]:
        """主要分割方法"""
        splits = self._split_tokens_with_separators(tokens)
        return self._merge_splits(splits)
    
    def _split_tokens_with_separators(self, tokens):
        """使用分隔符分割token"""
        # 按优先级查找分隔符并分割
        
    def _merge_splits(self, splits):
        """合并小片段为合适大小的分块"""
        # 智能合并逻辑
        
    def _enforce_overlap(self, chunks):
        """添加分块重叠"""
        # 重叠处理逻辑
```

## 📊 kv_store_text_chunks.json 文件结构分析

### 文件位置
`./mytest/kv_store_text_chunks.json`

### 数据结构详解

```json
{
  "chunk-{哈希ID}": {
    "tokens": 1200,                    // 分块的Token数量
    "content": "文档内容...",           // 分块的实际文本内容
    "chunk_order_index": 0,            // 在原文档中的分块顺序（从0开始）
    "full_doc_id": "doc-{文档哈希ID}"  // 原始文档的唯一标识符
  }
}
```

### 实际数据示例

```json
{
  "chunk-4fff7251c7c747260398bf3bac7f0a6f": {
    "tokens": 1200,
    "content": "曹操分兵收割军粮后，吕布陈宫看到曹操兵力分散...",
    "chunk_order_index": 0,
    "full_doc_id": "doc-5847bc92a3a6d4ae3daac921d8fcfe70"
  },
  "chunk-401872faa589e7456e18805b866a25e1": {
    "tokens": 1200,
    "content": "气，兴平二年（公元195年）二月干脆趁召开军事会议...",
    "chunk_order_index": 1,
    "full_doc_id": "doc-5847bc92a3a6d4ae3daac921d8fcfe70"
  },
  "chunk-36d6084250ac83de36e517351a390afd": {
    "tokens": 322,
    "content": "？这个逃亡中的小朝廷在如此窘迫的情况下...",
    "chunk_order_index": 2,
    "full_doc_id": "doc-5847bc92a3a6d4ae3daac921d8fcfe70"
  }
}
```

### 字段解析

| 字段名 | 类型 | 作用 | 示例 |
|--------|------|------|------|
| `哈希ID` | string | 分块的唯一标识符，格式: `chunk-{MD5哈希}` | `chunk-4fff7251c7c747260398bf3bac7f0a6f` |
| `tokens` | integer | 该分块包含的Token数量 | `1200` |
| `content` | string | 分块的文本内容 | `"曹操分兵收割军粮后..."` |
| `chunk_order_index` | integer | 在原文档中的顺序索引 | `0`, `1`, `2` |
| `full_doc_id` | string | 原始文档ID，格式: `doc-{MD5哈希}` | `doc-5847bc92a3a6d4ae3daac921d8fcfe70` |

## 🔄 分块过程在 insert 方法中的调用

### 在 GraphRAG.ainsert() 中的执行

```python
# 步骤2: 文本分块处理
inserting_chunks = get_chunks(
    new_docs=new_docs,                              # 输入文档
    chunk_func=self.chunk_func,                     # 分块函数（默认: chunking_by_token_size）
    overlap_token_size=self.chunk_overlap_token_size, # 重叠token数（默认: 100）
    max_token_size=self.chunk_token_size,           # 最大分块大小（默认: 1200）
)
```

### 配置参数

在 `GraphRAG` 类中的默认配置：

```python
@dataclass
class GraphRAG:
    # 分块相关配置
    chunk_func: Callable = chunking_by_token_size  # 分块函数
    chunk_token_size: int = 1200                   # 分块大小
    chunk_overlap_token_size: int = 100            # 重叠大小
    tiktoken_model_name: str = "gpt-4o"            # Token计算模型
```

## 🎯 分块算法对比

### Token 分块 vs 分隔符分块

| 特性 | Token分块 | 分隔符分块 |
|------|-----------|------------|
| **精确度** | 精确控制大小 | 根据语义边界 |
| **语义完整性** | 可能截断句子 | 保持语义完整 |
| **处理速度** | 快速 | 稍慢（需要查找分隔符） |
| **适用场景** | 严格大小控制 | 需要语义完整性 |
| **重叠处理** | 简单滑动窗口 | 智能边界对齐 |

### 实际分块结果对比

从 `kv_store_text_chunks.json` 可以看到，三国演义文档被分成了3个分块：

1. **分块1**: 1200 tokens - 从"曹操分兵收割军粮"开始
2. **分块2**: 1200 tokens - 从"气，兴平二年"开始（有重叠）
3. **分块3**: 322 tokens - 文档结尾部分

## 📚 文档分块的重要作用

### 1. 为后续处理做准备
- **实体提取**: LLM 分析每个分块提取实体和关系
- **向量化**: 将分块转换为向量用于相似度搜索
- **索引构建**: 建立分块到原文档的映射关系

### 2. 性能优化
- **并行处理**: 多个分块可以并行进行实体提取
- **内存控制**: 避免一次性加载过大的文档
- **API 调用优化**: 分块大小适合 LLM 的上下文窗口

### 3. 查询支持
- **精确检索**: 根据分块ID快速定位具体内容
- **相关性计算**: 基于分块级别的相似度匹配
- **上下文构建**: 为查询构建相关的文本上下文

## 🛠️ 自定义分块方法

### 如何替换分块函数

```python
# 自定义分块函数
def my_custom_chunking(tokens_list, doc_keys, tiktoken_model, **params):
    # 实现自己的分块逻辑
    pass

# 使用自定义分块函数
graph_func = GraphRAG(
    working_dir="./mytest",
    chunk_func=my_custom_chunking,  # 使用自定义函数
    chunk_token_size=800,          # 自定义分块大小
    chunk_overlap_token_size=50,   # 自定义重叠大小
)
```

### 示例：基于段落的分块

```python
def chunking_by_paragraphs(tokens_list, doc_keys, tiktoken_model, max_token_size=1024):
    """基于段落的分块方法"""
    results = []
    
    for index, tokens in enumerate(tokens_list):
        # 解码为文本
        text = tiktoken_model.decode(tokens)
        
        # 按段落分割
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        chunk_index = 0
        
        for paragraph in paragraphs:
            para_tokens = tiktoken_model.encode(paragraph)
            
            if len(tiktoken_model.encode(current_chunk + paragraph)) <= max_token_size:
                current_chunk += paragraph + '\n\n'
            else:
                if current_chunk:
                    results.append({
                        "tokens": len(tiktoken_model.encode(current_chunk)),
                        "content": current_chunk.strip(),
                        "chunk_order_index": chunk_index,
                        "full_doc_id": doc_keys[index],
                    })
                    chunk_index += 1
                current_chunk = paragraph + '\n\n'
        
        # 处理最后一个分块
        if current_chunk:
            results.append({
                "tokens": len(tiktoken_model.encode(current_chunk)),
                "content": current_chunk.strip(),
                "chunk_order_index": chunk_index,
                "full_doc_id": doc_keys[index],
            })
    
    return results
```

## 💡 总结

文本分块是 GraphRAG 系统的基础环节，它：

1. **位于关键路径**: 是 insert 方法的第二步，为后续所有处理奠定基础
2. **支持多种策略**: Token分块和分隔符分块，可根据需求选择
3. **性能优化**: 使用批量处理和多线程编码提高效率
4. **数据完整性**: 通过重叠机制避免重要信息丢失
5. **可扩展性**: 支持自定义分块函数，满足特殊需求

生成的 `kv_store_text_chunks.json` 文件是整个知识图谱构建过程的重要中间产物，为实体提取、关系识别和查询检索提供了结构化的文本基础。 