# 文档分块功能使用指南

## 为什么GraphRAG有这么多设置函数？

GraphRAG是一个**复杂的多组件系统**，类似于一个完整的"智能工厂"，需要协调多个子系统：

### 系统架构图
```
GraphRAG 完整系统
├── LLM调用系统 (需要API配置、模型选择、并发控制)
├── 文档处理系统 (文本分块、实体提取、关系识别)  ← 你想要的部分
├── 向量化系统 (嵌入模型、向量数据库、相似度计算)
├── 图存储系统 (图数据库、聚类算法、社区检测)
├── 缓存系统 (LLM响应缓存、持久化存储)
├── 查询系统 (本地查询、全局查询、朴素RAG)
└── 并发控制系统 (API限流、异步处理)
```

每个子系统都有自己的配置需求，这就导致了大量的设置函数。

## 文档分块功能核心代码

文档分块功能主要涉及以下核心代码：

### 1. 主要函数位置

| 函数名 | 文件位置 | 作用 |
|--------|----------|------|
| `chunking_by_token_size()` | `nano_graphrag/_op.py:32-60` | 基于Token数量分块 |
| `chunking_by_separator()` | `nano_graphrag/_op.py:62-90` | 基于分隔符分块 |
| `get_chunks()` | `nano_graphrag/_op.py:92-119` | 分块的统一入口函数 |
| `SeparatorSplitter` | `nano_graphrag/_splitter.py:3-95` | 分隔符分割器类 |

### 2. 核心依赖

- **tiktoken**: 用于Token编码/解码
- **hashlib**: 用于生成分块的唯一ID
- **typing**: 类型注解

### 3. 关键配置参数

```python
# 在GraphRAG中的配置
chunk_func: Callable = chunking_by_token_size  # 分块函数
chunk_token_size: int = 1200                   # 分块大小
chunk_overlap_token_size: int = 100            # 重叠大小
tiktoken_model_name: str = "gpt-4o"            # Token计算模型
```

## 独立的文档分块模块

我已经为您提取并重构了一个独立的文档分块模块 `document_chunker.py`，它包含了：

### ✅ 完整功能
- ✅ 基于Token数量的分块
- ✅ 基于分隔符的分块  
- ✅ 批量文档处理
- ✅ 结果导出（JSON格式）
- ✅ 与GraphRAG兼容的数据格式

### ✅ 零依赖于GraphRAG
- ✅ 只依赖tiktoken（Token处理）
- ✅ 不需要LLM API
- ✅ 不需要向量数据库
- ✅ 不需要图存储

## 使用方法

### 1. 基本使用

```python
from document_chunker import DocumentChunker

# 创建分块器
chunker = DocumentChunker(
    default_chunk_size=1200,    # 每个分块的token数量
    default_overlap_size=100    # 分块间的重叠token数量
)

# 分块单个文档
text = "你的长文档内容..."
chunks = chunker.chunk_text(text, method="token")

# 查看结果
for chunk in chunks:
    print(f"分块内容: {chunk.content[:100]}...")
    print(f"Token数量: {chunk.token_count}")
    print(f"分块ID: {chunk.hash_id}")
```

### 2. 批量处理文档

```python
# 准备多个文档
documents = {
    "doc1": "第一个文档的内容...",
    "doc2": "第二个文档的内容...",
    "doc3": "第三个文档的内容..."
}

# 批量分块
chunks = chunker.chunk_documents(documents, method="token")

# 导出为GraphRAG兼容格式
chunk_dict = chunker.export_chunks_to_dict(chunks)

# 保存到文件
chunker.save_chunks_to_file(chunks, "my_chunks.json")
```

### 3. 高级配置

```python
# 使用分隔符分块（更智能的分割）
chunks = chunker.chunk_text(
    text, 
    method="separator",
    max_token_size=800,
    overlap_token_size=80,
    separators=["\n\n", "\n", "。", ".", "！", "!"]  # 自定义分隔符
)

# 使用不同的模型计算Token
chunker = DocumentChunker(
    model_name="gpt-3.5-turbo",    # 或其他tiktoken支持的模型
    default_chunk_size=1500,
    default_overlap_size=150
)
```

## 与GraphRAG的对比

### GraphRAG完整流程
```python
# GraphRAG需要完整的系统初始化
graph_func = GraphRAG(working_dir="./mytest")  # 初始化所有组件
graph_func.insert(text)                        # 分块+实体提取+图构建+...
result = graph_func.query("问题")              # 复杂的查询流程
```

### 独立分块模块
```python
# 只做分块，简单直接
chunker = DocumentChunker()           # 只初始化分块器
chunks = chunker.chunk_text(text)     # 只做分块
# 可以将结果用于其他系统
```

## 性能对比

| 指标 | GraphRAG完整系统 | 独立分块模块 |
|------|-----------------|-------------|
| 初始化时间 | 2-5秒 | <0.1秒 |
| 内存使用 | 高（多个组件） | 低（仅分块器） |
| 依赖复杂度 | 高（LLM、向量DB等） | 低（仅tiktoken） |
| 学习成本 | 高 | 低 |
| 灵活性 | 低（预设流程） | 高（可自定义） |

## 实际应用场景

### 1. 预处理文档用于其他RAG系统
```python
# 为其他RAG系统准备分块数据
chunker = DocumentChunker()
chunks = chunker.chunk_text(long_document)

# 发送到你的向量数据库
for chunk in chunks:
    your_vector_db.add(chunk.content, chunk.hash_id)
```

### 2. 文档分析和统计
```python
# 分析文档的Token分布
chunks = chunker.chunk_text(document)
token_counts = [chunk.token_count for chunk in chunks]
print(f"平均分块大小: {sum(token_counts)/len(token_counts)}")
print(f"总分块数: {len(chunks)}")
```

### 3. 与现有系统集成
```python
# 生成与GraphRAG兼容的格式
chunk_dict = chunker.export_chunks_to_dict(chunks)

# 可以直接导入到GraphRAG的存储格式中
# 或用于其他需要相同数据格式的系统
```

## 定制化扩展

### 1. 自定义分块策略
```python
class CustomChunker(DocumentChunker):
    def custom_chunking_method(self, texts, doc_ids, **kwargs):
        # 实现你自己的分块逻辑
        # 例如：基于句子、段落、或特定标记的分块
        pass
```

### 2. 添加新的分隔符
```python
# 针对特定领域的分隔符
legal_separators = ["第一条", "第二条", "条款", "附录"]
chunker.default_separators.extend(legal_separators)
```

### 3. 输出格式自定义
```python
def export_to_csv(chunks):
    import csv
    with open('chunks.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['ID', 'Content', 'Tokens', 'Doc_ID'])
        for chunk in chunks:
            writer.writerow([chunk.hash_id, chunk.content, 
                           chunk.token_count, chunk.doc_id])
```

## 总结

### 🎯 何时使用独立分块模块
- ✅ 只需要文档分块功能
- ✅ 想要简单、轻量的解决方案
- ✅ 需要与现有系统集成
- ✅ 要自定义分块策略

### 🎯 何时使用GraphRAG完整系统
- ✅ 需要完整的知识图谱构建
- ✅ 需要复杂的实体关系提取
- ✅ 需要多种查询模式
- ✅ 想要开箱即用的完整解决方案

独立的文档分块模块给了您**最大的灵活性**，让您可以：
1. **渐进式采用**: 先使用分块功能，后续逐步集成其他功能
2. **系统解耦**: 分块不依赖于特定的LLM或向量数据库
3. **成本控制**: 避免不必要的API调用和资源消耗
4. **定制化**: 根据您的具体需求调整分块策略 