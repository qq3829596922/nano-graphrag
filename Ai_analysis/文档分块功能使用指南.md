# æ–‡æ¡£åˆ†å—åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## ä¸ºä»€ä¹ˆGraphRAGæœ‰è¿™ä¹ˆå¤šè®¾ç½®å‡½æ•°ï¼Ÿ

GraphRAGæ˜¯ä¸€ä¸ª**å¤æ‚çš„å¤šç»„ä»¶ç³»ç»Ÿ**ï¼Œç±»ä¼¼äºä¸€ä¸ªå®Œæ•´çš„"æ™ºèƒ½å·¥å‚"ï¼Œéœ€è¦åè°ƒå¤šä¸ªå­ç³»ç»Ÿï¼š

### ç³»ç»Ÿæ¶æ„å›¾
```
GraphRAG å®Œæ•´ç³»ç»Ÿ
â”œâ”€â”€ LLMè°ƒç”¨ç³»ç»Ÿ (éœ€è¦APIé…ç½®ã€æ¨¡å‹é€‰æ‹©ã€å¹¶å‘æ§åˆ¶)
â”œâ”€â”€ æ–‡æ¡£å¤„ç†ç³»ç»Ÿ (æ–‡æœ¬åˆ†å—ã€å®ä½“æå–ã€å…³ç³»è¯†åˆ«)  â† ä½ æƒ³è¦çš„éƒ¨åˆ†
â”œâ”€â”€ å‘é‡åŒ–ç³»ç»Ÿ (åµŒå…¥æ¨¡å‹ã€å‘é‡æ•°æ®åº“ã€ç›¸ä¼¼åº¦è®¡ç®—)
â”œâ”€â”€ å›¾å­˜å‚¨ç³»ç»Ÿ (å›¾æ•°æ®åº“ã€èšç±»ç®—æ³•ã€ç¤¾åŒºæ£€æµ‹)
â”œâ”€â”€ ç¼“å­˜ç³»ç»Ÿ (LLMå“åº”ç¼“å­˜ã€æŒä¹…åŒ–å­˜å‚¨)
â”œâ”€â”€ æŸ¥è¯¢ç³»ç»Ÿ (æœ¬åœ°æŸ¥è¯¢ã€å…¨å±€æŸ¥è¯¢ã€æœ´ç´ RAG)
â””â”€â”€ å¹¶å‘æ§åˆ¶ç³»ç»Ÿ (APIé™æµã€å¼‚æ­¥å¤„ç†)
```

æ¯ä¸ªå­ç³»ç»Ÿéƒ½æœ‰è‡ªå·±çš„é…ç½®éœ€æ±‚ï¼Œè¿™å°±å¯¼è‡´äº†å¤§é‡çš„è®¾ç½®å‡½æ•°ã€‚

## æ–‡æ¡£åˆ†å—åŠŸèƒ½æ ¸å¿ƒä»£ç 

æ–‡æ¡£åˆ†å—åŠŸèƒ½ä¸»è¦æ¶‰åŠä»¥ä¸‹æ ¸å¿ƒä»£ç ï¼š

### 1. ä¸»è¦å‡½æ•°ä½ç½®

| å‡½æ•°å | æ–‡ä»¶ä½ç½® | ä½œç”¨ |
|--------|----------|------|
| `chunking_by_token_size()` | `nano_graphrag/_op.py:32-60` | åŸºäºTokenæ•°é‡åˆ†å— |
| `chunking_by_separator()` | `nano_graphrag/_op.py:62-90` | åŸºäºåˆ†éš”ç¬¦åˆ†å— |
| `get_chunks()` | `nano_graphrag/_op.py:92-119` | åˆ†å—çš„ç»Ÿä¸€å…¥å£å‡½æ•° |
| `SeparatorSplitter` | `nano_graphrag/_splitter.py:3-95` | åˆ†éš”ç¬¦åˆ†å‰²å™¨ç±» |

### 2. æ ¸å¿ƒä¾èµ–

- **tiktoken**: ç”¨äºTokenç¼–ç /è§£ç 
- **hashlib**: ç”¨äºç”Ÿæˆåˆ†å—çš„å”¯ä¸€ID
- **typing**: ç±»å‹æ³¨è§£

### 3. å…³é”®é…ç½®å‚æ•°

```python
# åœ¨GraphRAGä¸­çš„é…ç½®
chunk_func: Callable = chunking_by_token_size  # åˆ†å—å‡½æ•°
chunk_token_size: int = 1200                   # åˆ†å—å¤§å°
chunk_overlap_token_size: int = 100            # é‡å å¤§å°
tiktoken_model_name: str = "gpt-4o"            # Tokenè®¡ç®—æ¨¡å‹
```

## ç‹¬ç«‹çš„æ–‡æ¡£åˆ†å—æ¨¡å—

æˆ‘å·²ç»ä¸ºæ‚¨æå–å¹¶é‡æ„äº†ä¸€ä¸ªç‹¬ç«‹çš„æ–‡æ¡£åˆ†å—æ¨¡å— `document_chunker.py`ï¼Œå®ƒåŒ…å«äº†ï¼š

### âœ… å®Œæ•´åŠŸèƒ½
- âœ… åŸºäºTokenæ•°é‡çš„åˆ†å—
- âœ… åŸºäºåˆ†éš”ç¬¦çš„åˆ†å—  
- âœ… æ‰¹é‡æ–‡æ¡£å¤„ç†
- âœ… ç»“æœå¯¼å‡ºï¼ˆJSONæ ¼å¼ï¼‰
- âœ… ä¸GraphRAGå…¼å®¹çš„æ•°æ®æ ¼å¼

### âœ… é›¶ä¾èµ–äºGraphRAG
- âœ… åªä¾èµ–tiktokenï¼ˆTokenå¤„ç†ï¼‰
- âœ… ä¸éœ€è¦LLM API
- âœ… ä¸éœ€è¦å‘é‡æ•°æ®åº“
- âœ… ä¸éœ€è¦å›¾å­˜å‚¨

## ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨

```python
from document_chunker import DocumentChunker

# åˆ›å»ºåˆ†å—å™¨
chunker = DocumentChunker(
    default_chunk_size=1200,    # æ¯ä¸ªåˆ†å—çš„tokenæ•°é‡
    default_overlap_size=100    # åˆ†å—é—´çš„é‡å tokenæ•°é‡
)

# åˆ†å—å•ä¸ªæ–‡æ¡£
text = "ä½ çš„é•¿æ–‡æ¡£å†…å®¹..."
chunks = chunker.chunk_text(text, method="token")

# æŸ¥çœ‹ç»“æœ
for chunk in chunks:
    print(f"åˆ†å—å†…å®¹: {chunk.content[:100]}...")
    print(f"Tokenæ•°é‡: {chunk.token_count}")
    print(f"åˆ†å—ID: {chunk.hash_id}")
```

### 2. æ‰¹é‡å¤„ç†æ–‡æ¡£

```python
# å‡†å¤‡å¤šä¸ªæ–‡æ¡£
documents = {
    "doc1": "ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å†…å®¹...",
    "doc2": "ç¬¬äºŒä¸ªæ–‡æ¡£çš„å†…å®¹...",
    "doc3": "ç¬¬ä¸‰ä¸ªæ–‡æ¡£çš„å†…å®¹..."
}

# æ‰¹é‡åˆ†å—
chunks = chunker.chunk_documents(documents, method="token")

# å¯¼å‡ºä¸ºGraphRAGå…¼å®¹æ ¼å¼
chunk_dict = chunker.export_chunks_to_dict(chunks)

# ä¿å­˜åˆ°æ–‡ä»¶
chunker.save_chunks_to_file(chunks, "my_chunks.json")
```

### 3. é«˜çº§é…ç½®

```python
# ä½¿ç”¨åˆ†éš”ç¬¦åˆ†å—ï¼ˆæ›´æ™ºèƒ½çš„åˆ†å‰²ï¼‰
chunks = chunker.chunk_text(
    text, 
    method="separator",
    max_token_size=800,
    overlap_token_size=80,
    separators=["\n\n", "\n", "ã€‚", ".", "ï¼", "!"]  # è‡ªå®šä¹‰åˆ†éš”ç¬¦
)

# ä½¿ç”¨ä¸åŒçš„æ¨¡å‹è®¡ç®—Token
chunker = DocumentChunker(
    model_name="gpt-3.5-turbo",    # æˆ–å…¶ä»–tiktokenæ”¯æŒçš„æ¨¡å‹
    default_chunk_size=1500,
    default_overlap_size=150
)
```

## ä¸GraphRAGçš„å¯¹æ¯”

### GraphRAGå®Œæ•´æµç¨‹
```python
# GraphRAGéœ€è¦å®Œæ•´çš„ç³»ç»Ÿåˆå§‹åŒ–
graph_func = GraphRAG(working_dir="./mytest")  # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
graph_func.insert(text)                        # åˆ†å—+å®ä½“æå–+å›¾æ„å»º+...
result = graph_func.query("é—®é¢˜")              # å¤æ‚çš„æŸ¥è¯¢æµç¨‹
```

### ç‹¬ç«‹åˆ†å—æ¨¡å—
```python
# åªåšåˆ†å—ï¼Œç®€å•ç›´æ¥
chunker = DocumentChunker()           # åªåˆå§‹åŒ–åˆ†å—å™¨
chunks = chunker.chunk_text(text)     # åªåšåˆ†å—
# å¯ä»¥å°†ç»“æœç”¨äºå…¶ä»–ç³»ç»Ÿ
```

## æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | GraphRAGå®Œæ•´ç³»ç»Ÿ | ç‹¬ç«‹åˆ†å—æ¨¡å— |
|------|-----------------|-------------|
| åˆå§‹åŒ–æ—¶é—´ | 2-5ç§’ | <0.1ç§’ |
| å†…å­˜ä½¿ç”¨ | é«˜ï¼ˆå¤šä¸ªç»„ä»¶ï¼‰ | ä½ï¼ˆä»…åˆ†å—å™¨ï¼‰ |
| ä¾èµ–å¤æ‚åº¦ | é«˜ï¼ˆLLMã€å‘é‡DBç­‰ï¼‰ | ä½ï¼ˆä»…tiktokenï¼‰ |
| å­¦ä¹ æˆæœ¬ | é«˜ | ä½ |
| çµæ´»æ€§ | ä½ï¼ˆé¢„è®¾æµç¨‹ï¼‰ | é«˜ï¼ˆå¯è‡ªå®šä¹‰ï¼‰ |

## å®é™…åº”ç”¨åœºæ™¯

### 1. é¢„å¤„ç†æ–‡æ¡£ç”¨äºå…¶ä»–RAGç³»ç»Ÿ
```python
# ä¸ºå…¶ä»–RAGç³»ç»Ÿå‡†å¤‡åˆ†å—æ•°æ®
chunker = DocumentChunker()
chunks = chunker.chunk_text(long_document)

# å‘é€åˆ°ä½ çš„å‘é‡æ•°æ®åº“
for chunk in chunks:
    your_vector_db.add(chunk.content, chunk.hash_id)
```

### 2. æ–‡æ¡£åˆ†æå’Œç»Ÿè®¡
```python
# åˆ†ææ–‡æ¡£çš„Tokenåˆ†å¸ƒ
chunks = chunker.chunk_text(document)
token_counts = [chunk.token_count for chunk in chunks]
print(f"å¹³å‡åˆ†å—å¤§å°: {sum(token_counts)/len(token_counts)}")
print(f"æ€»åˆ†å—æ•°: {len(chunks)}")
```

### 3. ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ
```python
# ç”Ÿæˆä¸GraphRAGå…¼å®¹çš„æ ¼å¼
chunk_dict = chunker.export_chunks_to_dict(chunks)

# å¯ä»¥ç›´æ¥å¯¼å…¥åˆ°GraphRAGçš„å­˜å‚¨æ ¼å¼ä¸­
# æˆ–ç”¨äºå…¶ä»–éœ€è¦ç›¸åŒæ•°æ®æ ¼å¼çš„ç³»ç»Ÿ
```

## å®šåˆ¶åŒ–æ‰©å±•

### 1. è‡ªå®šä¹‰åˆ†å—ç­–ç•¥
```python
class CustomChunker(DocumentChunker):
    def custom_chunking_method(self, texts, doc_ids, **kwargs):
        # å®ç°ä½ è‡ªå·±çš„åˆ†å—é€»è¾‘
        # ä¾‹å¦‚ï¼šåŸºäºå¥å­ã€æ®µè½ã€æˆ–ç‰¹å®šæ ‡è®°çš„åˆ†å—
        pass
```

### 2. æ·»åŠ æ–°çš„åˆ†éš”ç¬¦
```python
# é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åˆ†éš”ç¬¦
legal_separators = ["ç¬¬ä¸€æ¡", "ç¬¬äºŒæ¡", "æ¡æ¬¾", "é™„å½•"]
chunker.default_separators.extend(legal_separators)
```

### 3. è¾“å‡ºæ ¼å¼è‡ªå®šä¹‰
```python
def export_to_csv(chunks):
    import csv
    with open('chunks.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['ID', 'Content', 'Tokens', 'Doc_ID'])
        for chunk in chunks:
            writer.writerow([chunk.hash_id, chunk.content, 
                           chunk.token_count, chunk.doc_id])
```

## æ€»ç»“

### ğŸ¯ ä½•æ—¶ä½¿ç”¨ç‹¬ç«‹åˆ†å—æ¨¡å—
- âœ… åªéœ€è¦æ–‡æ¡£åˆ†å—åŠŸèƒ½
- âœ… æƒ³è¦ç®€å•ã€è½»é‡çš„è§£å†³æ–¹æ¡ˆ
- âœ… éœ€è¦ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ
- âœ… è¦è‡ªå®šä¹‰åˆ†å—ç­–ç•¥

### ğŸ¯ ä½•æ—¶ä½¿ç”¨GraphRAGå®Œæ•´ç³»ç»Ÿ
- âœ… éœ€è¦å®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»º
- âœ… éœ€è¦å¤æ‚çš„å®ä½“å…³ç³»æå–
- âœ… éœ€è¦å¤šç§æŸ¥è¯¢æ¨¡å¼
- âœ… æƒ³è¦å¼€ç®±å³ç”¨çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

ç‹¬ç«‹çš„æ–‡æ¡£åˆ†å—æ¨¡å—ç»™äº†æ‚¨**æœ€å¤§çš„çµæ´»æ€§**ï¼Œè®©æ‚¨å¯ä»¥ï¼š
1. **æ¸è¿›å¼é‡‡ç”¨**: å…ˆä½¿ç”¨åˆ†å—åŠŸèƒ½ï¼Œåç»­é€æ­¥é›†æˆå…¶ä»–åŠŸèƒ½
2. **ç³»ç»Ÿè§£è€¦**: åˆ†å—ä¸ä¾èµ–äºç‰¹å®šçš„LLMæˆ–å‘é‡æ•°æ®åº“
3. **æˆæœ¬æ§åˆ¶**: é¿å…ä¸å¿…è¦çš„APIè°ƒç”¨å’Œèµ„æºæ¶ˆè€—
4. **å®šåˆ¶åŒ–**: æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚è°ƒæ•´åˆ†å—ç­–ç•¥ 